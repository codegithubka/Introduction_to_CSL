{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (10, 6),\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"axes.titlesize\": 16,\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"lines.linewidth\": 2,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gillespie's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "\n",
    "def sir_rhs(t, y, beta, gamma, mu, N):\n",
    "    \"\"\"\n",
    "    Frequency-dependent SIR with COUNTS (X,Y,Z) and fixed population N.\n",
    "    dX/dt = -beta * X * Y / N\n",
    "    dY/dt =  beta * X * Y / N - gamma * Y\n",
    "    dZ/dt =  gamma * Y\n",
    "    \"\"\"\n",
    "    X, Y, Z = y\n",
    "\n",
    "    dX = mu * N - beta * X * Y / N - mu * X\n",
    "    dY = beta * X * Y / N - gamma * Y - mu * Y\n",
    "    dZ = gamma * Y - mu * Z\n",
    "    return [dX, dY, dZ]\n",
    "\n",
    "\n",
    "def integrate_sir(\n",
    "    beta, gamma, mu, N, X0, Y0, Z0, t_span=(0.0, 160.0), num_points=1000, method=\"RK45\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal integrator wrapper for counts-based frequency-dependent SIR.\n",
    "    Returns dict with t, X, Y, Z (arrays).\n",
    "    \"\"\"\n",
    "\n",
    "    y0 = [float(X0), float(Y0), float(Z0)]\n",
    "    t_eval = np.linspace(t_span[0], t_span[1], num_points)\n",
    "\n",
    "    sol = solve_ivp(\n",
    "        lambda t, y: sir_rhs(t, y, beta, gamma, mu, N),\n",
    "        t_span,\n",
    "        y0,\n",
    "        t_eval=t_eval,\n",
    "        method=method,\n",
    "        rtol=1e-8,\n",
    "        atol=1e-10,\n",
    "    )\n",
    "\n",
    "    # Numerical safety: clip tiny negatives from solver noise\n",
    "\n",
    "    Y = np.clip(sol.y, 0.0, None)\n",
    "    return {\"t\": sol.t, \"X\": Y[0], \"Y\": Y[1], \"Z\": Y[2]}\n",
    "\n",
    "\n",
    "def gillespie_sir(beta, gamma, mu, N, X0, Y0, Z0, tmax, rng=None):\n",
    "    \"\"\"\n",
    "    infectionL (X - 1, Y + 1), rate_1 = beta * X * Y / N\n",
    "    recovery: (Y - 1, Z + 1), rate_2 = gamma * Y\n",
    "    birth:  (X + 1), rate_3 = mu * N\n",
    "    deathX: (X - 1), rate_4 = mu * X\n",
    "    deathY: (Y - 1), rate_5 = mu * Y\n",
    "    deathZ: (Z - 1), rate_6 = mu * Z\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # Validate & coerce to ints\n",
    "\n",
    "    X, Y, Z = int(X0), int(Y0), int(Z0)\n",
    "    t = 0.0\n",
    "\n",
    "    times = [0.0]\n",
    "    X_hist, Y_hist, Z_hist = [X], [Y], [Z]\n",
    "\n",
    "    while t < tmax and Y > 0:\n",
    "        rate_I = beta * X * Y / N  # infection\n",
    "        rate_R = gamma * Y\n",
    "        rate_B = mu * N  # birth\n",
    "        rate_DX = mu * X  # death of susceptible\n",
    "        rate_DY = mu * Y  # death of infected\n",
    "        rate_DZ = mu * Z  # death of recovered\n",
    "        total_rate = rate_I + rate_R + rate_B + rate_DX + rate_DY + rate_DZ\n",
    "\n",
    "        # exponential waiting time\n",
    "        t += -np.log(rng.random()) / total_rate\n",
    "\n",
    "        r = rng.random() * total_rate\n",
    "\n",
    "        if r < rate_I:\n",
    "            if X > 0:\n",
    "                X -= 1\n",
    "                Y += 1\n",
    "        elif r < rate_I + rate_R:\n",
    "            if Y > 0:\n",
    "                Y -= 1\n",
    "                Z += 1\n",
    "        elif r < rate_I + rate_R + rate_B:\n",
    "            X += 1\n",
    "        elif r < rate_I + rate_R + rate_B + rate_DX:\n",
    "            if X > 0:\n",
    "                X -= 1\n",
    "        elif r < rate_I + rate_R + rate_B + rate_DX + rate_DY:\n",
    "            if Y > 0:\n",
    "                Y -= 1\n",
    "        else:\n",
    "            if Z > 0:\n",
    "                Z -= 1\n",
    "\n",
    "        times.append(t)\n",
    "        X_hist.append(X)\n",
    "        Y_hist.append(Y)\n",
    "        Z_hist.append(Z)\n",
    "\n",
    "    return np.array(times), np.array(X_hist), np.array(Y_hist), np.array(Z_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_step(times, values, t_grid):\n",
    "    \"\"\"Piecewise-constant resampling of an event-driven trajectory.\"\"\"\n",
    "\n",
    "    out = np.empty_like(t_grid, dtype=float)\n",
    "    j = 0\n",
    "    curr = values[0]\n",
    "    for k, tk in enumerate(t_grid):\n",
    "        while j + 1 < len(times) and times[j + 1] <= tk:\n",
    "            j += 1\n",
    "            curr = values[j]\n",
    "        out[k] = curr\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare stochastic and deterministic SIR models\n",
    "\n",
    "beta = 0.5\n",
    "gamma = 0.1\n",
    "mu = 1 / 50\n",
    "N = 1000\n",
    "I0 = 10\n",
    "S0 = N - I0\n",
    "R0 = 0\n",
    "\n",
    "t_span = (0.0, 160.0)\n",
    "num_points = 1000\n",
    "t_grid = np.linspace(t_span[0], t_span[1], num_points)\n",
    "\n",
    "# Deterministic solution\n",
    "det = integrate_sir(\n",
    "    beta, gamma, mu, N, S0, I0, R0, t_span=t_span, num_points=num_points\n",
    ")\n",
    "\n",
    "# Stochastic replicates\n",
    "n_runs = 200\n",
    "I_paths = np.zeros((n_runs, num_points), dtype=float)\n",
    "\n",
    "for r in range(n_runs):\n",
    "    rng = np.random.default_rng(12345 + r)  # reproducible variety\n",
    "    times, S_hist, I_hist, R_hist = gillespie_sir(\n",
    "        beta, gamma, mu, N, S0, I0, R0, tmax=t_span[1], rng=rng\n",
    "    )\n",
    "    I_paths[r, :] = resample_step(times, I_hist, t_grid)\n",
    "\n",
    "I_mean = I_paths.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = 10  # number of stochastic runs to show\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for r in range(n_show):\n",
    "    ax.plot(t_grid, I_paths[r, :], color=\"tab:blue\", alpha=0.3, lw=1)\n",
    "\n",
    "# Deterministic\n",
    "ax.plot(det[\"t\"], det[\"Y\"], \"k--\", lw=2.5, label=\"Deterministic I(t)\")\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Infected (count)\")\n",
    "ax.set_title(\"Frequency-dependent SIR: Deterministic vs Stochastic\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Variability and Negative Co-variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble(\n",
    "    beta,\n",
    "    gamma,\n",
    "    mu,\n",
    "    N,\n",
    "    X0,\n",
    "    Y0,\n",
    "    Z0,\n",
    "    t_span=(0.0, 160.0),\n",
    "    num_points=1000,\n",
    "    n_runs=200,\n",
    "    seed0=12345,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs many Gillespie simulations and the deterministic ODE\n",
    "    using counts (X,Y,Z). Returns t grid, deterministic curves,\n",
    "    and stacked X/Y/Z paths.\n",
    "    \"\"\"\n",
    "    # Use the counts-based integrator so keys are X,Y,Z\n",
    "    det = integrate_sir(\n",
    "        beta, gamma, mu, N, X0, Y0, Z0, t_span=t_span, num_points=num_points\n",
    "    )\n",
    "    t_grid = det[\"t\"]\n",
    "\n",
    "    X_paths = np.zeros((n_runs, num_points))\n",
    "    Y_paths = np.zeros((n_runs, num_points))\n",
    "    Z_paths = np.zeros((n_runs, num_points))\n",
    "\n",
    "    for r in range(n_runs):\n",
    "        rng = np.random.default_rng(seed0 + r)\n",
    "        times, X_hist, Y_hist, Z_hist = gillespie_sir(\n",
    "            beta, gamma, mu, N, X0, Y0, Z0, tmax=t_span[1], rng=rng\n",
    "        )\n",
    "        X_paths[r, :] = resample_step(times, X_hist, t_grid)\n",
    "        Y_paths[r, :] = resample_step(times, Y_hist, t_grid)\n",
    "        Z_paths[r, :] = resample_step(times, Z_hist, t_grid)\n",
    "\n",
    "    return t_grid, det, X_paths, Y_paths, Z_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_stats(X_paths, Y_paths, Z_paths):\n",
    "    \"\"\"Compute time-wise mean/variance for X, Y, Z, and Cov[X,Y] (counts).\"\"\"\n",
    "    X_mean = X_paths.mean(axis=0)\n",
    "    Y_mean = Y_paths.mean(axis=0)\n",
    "    Z_mean = Z_paths.mean(axis=0)\n",
    "\n",
    "    X_var = X_paths.var(axis=0, ddof=1)\n",
    "    Y_var = Y_paths.var(axis=0, ddof=1)\n",
    "    Z_var = Z_paths.var(axis=0, ddof=1)\n",
    "\n",
    "    XY_mean = (X_paths * Y_paths).mean(axis=0)\n",
    "    XY_cov = XY_mean - X_mean * Y_mean\n",
    "\n",
    "    return {\n",
    "        \"X_mean\": X_mean,\n",
    "        \"Y_mean\": Y_mean,\n",
    "        \"Z_mean\": Z_mean,\n",
    "        \"X_var\": X_var,\n",
    "        \"Y_var\": Y_var,\n",
    "        \"Z_var\": Z_var,\n",
    "        \"XY_cov\": XY_cov,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "beta = 0.5\n",
    "gamma = 0.1\n",
    "mu = 1 / 50\n",
    "t_span = (0, 160)\n",
    "num_points = 1000\n",
    "n_runs = 200\n",
    "\n",
    "N_list = [500, 1000, 2000, 5000, 10000]\n",
    "I0_fixed = 5\n",
    "i0_frac = I0_fixed / 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_var, ax_cov) = plt.subplots(1, 2)\n",
    "\n",
    "Nmin, Nmax = min(N_list), max(N_list)\n",
    "\n",
    "for N in N_list:\n",
    "    # fixed initial fraction\n",
    "    Y0 = max(1, int(ceil(i0_frac * N)))\n",
    "    X0, Z0 = N - Y0, 0\n",
    "\n",
    "    t, det, X_paths, Y_paths, Z_paths = run_ensemble(\n",
    "        beta,\n",
    "        gamma,\n",
    "        mu,\n",
    "        N,\n",
    "        X0,\n",
    "        Y0,\n",
    "        Z0,\n",
    "        t_span=t_span,\n",
    "        num_points=num_points,\n",
    "        n_runs=n_runs,\n",
    "        seed0=6000,\n",
    "    )\n",
    "\n",
    "    # convert to fractions\n",
    "    y_paths = Y_paths / N\n",
    "    x_paths = X_paths / N\n",
    "\n",
    "    # normalized stats\n",
    "    y_var = y_paths.var(axis=0, ddof=1)\n",
    "    xy_mean = (x_paths * y_paths).mean(axis=0)\n",
    "    xy_cov = xy_mean - x_paths.mean(axis=0) * y_paths.mean(axis=0)\n",
    "\n",
    "    color = plt.cm.viridis((N - Nmin) / (Nmax - Nmin))\n",
    "\n",
    "    # Variance of infected fraction\n",
    "    ax_var.plot(t, y_var, color=color, label=f\"N={N}\")\n",
    "\n",
    "    # Covariance of susceptible vs infected fractions\n",
    "    ax_cov.plot(t, xy_cov, color=color, label=f\"N={N}\")\n",
    "\n",
    "ax_cov.axhline(0, color=\"k\", lw=1)\n",
    "\n",
    "ax_var.set_title(\"Variance of Infected Fraction $y(t)$\")\n",
    "ax_var.set_xlabel(\"Time\")\n",
    "ax_var.set_ylabel(\"Variance\")\n",
    "\n",
    "ax_cov.set_title(\"Covariance of Susceptible and Infected Fractions\")\n",
    "ax_cov.set_xlabel(\"Time\")\n",
    "ax_cov.set_ylabel(\"Covariance\")\n",
    "\n",
    "ax_var.legend(title=\"Population Size\")\n",
    "ax_cov.legend(title=\"Population Size\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "def find_sir_equilibrium(beta, gamma, mu, N):\n",
    "    \"\"\"Find the endemic equilibrium for SIR with demography.\"\"\"\n",
    "    R0 = beta / (gamma + mu)\n",
    "    if R0 <= 1.0:\n",
    "        return (N, 0, 0)\n",
    "    else:\n",
    "        X_eq = N / R0\n",
    "        Y_eq = mu * N * (R0 - 1) / beta\n",
    "        Z_eq = N - X_eq - Y_eq\n",
    "        return (X_eq, Y_eq, Z_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(y_signal, dt):\n",
    "    \"\"\"\n",
    "    FFT-based PSD with correct one-sided normalization.\n",
    "    Uses second half (steady-state), mean removal, Hann window.\n",
    "    Returns freqs (1/time) and PSD with units 'power per (1/time)'.\n",
    "    \"\"\"\n",
    "    # Steady-state half\n",
    "    mid = len(y_signal) // 2\n",
    "    y_steady = y_signal[mid:]\n",
    "\n",
    "    # Detrend (mean) and window\n",
    "    y_centered = y_steady - np.mean(y_steady)\n",
    "    window = np.hanning(len(y_centered))\n",
    "    y_windowed = y_centered * window\n",
    "\n",
    "    # FFT\n",
    "    fft_vals = np.fft.rfft(y_windowed)\n",
    "    freqs = np.fft.rfftfreq(len(y_windowed), d=dt)\n",
    "\n",
    "    Fs = 1.0 / dt\n",
    "    psd = (np.abs(fft_vals) ** 2) / (Fs * np.sum(window**2))\n",
    "\n",
    "    if len(psd) > 2:\n",
    "        psd[1:-1] *= 2\n",
    "\n",
    "    return freqs, psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_resonance(beta, gamma, mu, N, n_runs=30, tmax=400):\n",
    "    \"\"\"\n",
    "    Analyze stochastic resonance: run simulations and compute averaged PSD.\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    t_grid = np.linspace(0, tmax, 4000)\n",
    "    dt = t_grid[1] - t_grid[0]\n",
    "    X_eq, Y_eq, Z_eq = find_sir_equilibrium(beta, gamma, mu, N)\n",
    "\n",
    "    X0, Y0, Z0 = int(X_eq), max(1, int(Y_eq)), int(Z_eq)\n",
    "\n",
    "    all_psds = []\n",
    "    all_trajectories = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        rng = np.random.default_rng(1000 + run)\n",
    "        times, X_hist, Y_hist, Z_hist = gillespie_sir(\n",
    "            beta, gamma, mu, N, X0, Y0, Z0, tmax, rng=rng\n",
    "        )\n",
    "        Y_uniform = resample_step(times, Y_hist, t_grid)\n",
    "        all_trajectories.append(Y_uniform)\n",
    "\n",
    "        freqs, psd = compute_psd(Y_uniform / N, dt)\n",
    "        all_psds.append(psd)\n",
    "\n",
    "    avg_psd = np.mean(all_psds, axis=0)\n",
    "\n",
    "    f_nyq = 0.5 / dt\n",
    "    f_low = 1.0 / tmax\n",
    "    f_high = min(0.1, 0.9 * f_nyq)\n",
    "    mask = (freqs >= f_low) & (freqs <= f_high)\n",
    "\n",
    "    peak_freq = None\n",
    "    peak_power = None\n",
    "    if np.any(mask):\n",
    "        prom = np.percentile(avg_psd[mask], 85)\n",
    "        peaks, properties = find_peaks(\n",
    "            avg_psd[mask], prominence=max(1e-12, prom), width=1\n",
    "        )\n",
    "        if len(peaks) > 0:\n",
    "            masked_psd = avg_psd[mask]\n",
    "            best = peaks[np.argmax(masked_psd[peaks])]\n",
    "            peak_freq = freqs[mask][best]\n",
    "            peak_power = masked_psd[best]\n",
    "\n",
    "    return {\n",
    "        \"freqs\": freqs,\n",
    "        \"avg_psd\": avg_psd,\n",
    "        \"peak_freq\": peak_freq,\n",
    "        \"peak_power\": peak_power,\n",
    "        \"trajectories\": all_trajectories,\n",
    "        \"t_grid\": t_grid,\n",
    "        \"Y_eq\": Y_eq,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "mu = 1 / 50\n",
    "N_values = [500, 1000, 5000, 10000]\n",
    "beta = 0.5\n",
    "\n",
    "fig, axes = plt.subplots(2, len(N_values), figsize=(15, 7))\n",
    "\n",
    "resonance_results = []\n",
    "for idx, N in enumerate(N_values):\n",
    "    res = analyze_resonance(beta, gamma, mu, N, n_runs=20, tmax=600)\n",
    "    resonance_results.append(res)\n",
    "\n",
    "    # Top: Sample trajectories\n",
    "    ax = axes[0, idx]\n",
    "    for i in range(min(3, len(res[\"trajectories\"]))):\n",
    "        ax.plot(res[\"t_grid\"], res[\"trajectories\"][i], alpha=0.4, lw=0.8)\n",
    "    ax.axhline(res[\"Y_eq\"], ls=\"--\", lw=1.0, alpha=0.7, color=\"red\", label=\"Y*\")\n",
    "    ax.set_title(f\"N = {N}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Y(t)\")\n",
    "    ax.set_xlim([300, 600])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Bottom: Power spectrum\n",
    "    ax = axes[1, idx]\n",
    "    mask = (res[\"freqs\"] > 0) & (res[\"freqs\"] < 0.1)\n",
    "    ax.semilogy(res[\"freqs\"][mask], res[\"avg_psd\"][mask], \"b-\", lw=1.5)\n",
    "\n",
    "    if res[\"peak_freq\"] is not None:\n",
    "        ax.plot(res[\"peak_freq\"], res[\"peak_power\"], \"ro\", markersize=8)\n",
    "        ax.annotate(\n",
    "            f\"f={res['peak_freq']:.3f}\",\n",
    "            xy=(res[\"peak_freq\"], res[\"peak_power\"]),\n",
    "            xytext=(5, 5),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Frequency\")\n",
    "    ax.set_ylabel(\"Power\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Stochastic Resonance: Effect of Population Size N\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for N, res in zip(N_values, resonance_results):\n",
    "    f_low, f_high = 1 / res[\"t_grid\"][-1], 0.1\n",
    "    mask = (res[\"freqs\"] >= f_low) & (res[\"freqs\"] <= f_high)\n",
    "    ax.semilogy(res[\"freqs\"][mask], res[\"avg_psd\"][mask], lw=1.5, label=f\"N={N}\")\n",
    "    if res[\"peak_freq\"] is not None:\n",
    "        ax.plot(res[\"peak_freq\"], res[\"peak_power\"], \"o\")\n",
    "\n",
    "ax.set_xlabel(\"Frequency (1/time)\")\n",
    "ax.set_ylabel(\"Power / (1/time)\")\n",
    "ax.set_title(\"Averaged PSD (overlay across N)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_fixed = 1000\n",
    "beta_grid = np.arange(0.1, 0.71, 0.1)\n",
    "psd_by_beta = []\n",
    "\n",
    "for b in beta_grid:\n",
    "    res = analyze_resonance(b, gamma, mu, N_fixed, n_runs=20, tmax=600)\n",
    "    psd_by_beta.append(res)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for b, res in zip(beta_grid, psd_by_beta):\n",
    "\n",
    "    f_low, f_high = 1 / res[\"t_grid\"][-1], 0.1\n",
    "    mask = (res[\"freqs\"] >= f_low) & (res[\"freqs\"] <= f_high)\n",
    "    ax.semilogy(res[\"freqs\"][mask], res[\"avg_psd\"][mask], lw=1.5, label=f\"β={b:.2f}\")\n",
    "    if res[\"peak_freq\"] is not None:\n",
    "        ax.plot(res[\"peak_freq\"], res[\"peak_power\"], \"o\")\n",
    "\n",
    "ax.set_xlabel(\"Frequency\")\n",
    "ax.set_ylabel(\"Power\")\n",
    "ax.set_title(f\"Averaged PSD (overlay across β, N={N_fixed})\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(ncol=2, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def analyze_transients_around_eq(\n",
    "    beta, gamma, mu, N, n_runs=40, tmax=800, num_points=2000, burn_frac=0.5, seed0=2000\n",
    "):\n",
    "    eq = find_sir_equilibrium(beta, gamma, mu, N)\n",
    "    if eq is None:\n",
    "        return np.nan\n",
    "    X_star, Y_star, Z_star = eq\n",
    "\n",
    "    X0 = max(0, int(round(X_star)))\n",
    "    Y0 = max(1, int(round(Y_star)))\n",
    "    Z0 = max(0, int(round(Z_star)))\n",
    "\n",
    "    t_grid = np.linspace(0.0, tmax, num_points)\n",
    "    burn = int(burn_frac * num_points)\n",
    "\n",
    "    overs = []\n",
    "    for r in range(n_runs):\n",
    "        rng = np.random.default_rng(seed0 + r)\n",
    "        times, X_hist, Y_hist, Z_hist = gillespie_sir(\n",
    "            beta, gamma, mu, N, X0, Y0, Z0, tmax=tmax, rng=rng\n",
    "        )\n",
    "        Y_uni = resample_step(times, Y_hist, t_grid)  # counts\n",
    "        peak_ss = float(np.max(Y_uni[burn:]))  # stationary window only\n",
    "        os = max(0.0, (peak_ss - Y_star) / N)\n",
    "        overs.append(os)\n",
    "\n",
    "    return float(np.mean(overs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_mean_overshoot(\n",
    "    N_grid,\n",
    "    R0_grid,\n",
    "    gamma,\n",
    "    mu,\n",
    "    n_runs=40,\n",
    "    tmax=800,\n",
    "    num_points=2000,\n",
    "    burn_frac=0.5,\n",
    "    n_jobs=-1,\n",
    "):\n",
    "    params = [(N, R0) for N in N_grid for R0 in R0_grid]\n",
    "\n",
    "    def run_point(N, R0):\n",
    "        if R0 <= 1.0:\n",
    "            return np.nan\n",
    "        beta = R0 * (gamma + mu)\n",
    "        return analyze_transients_around_eq(\n",
    "            beta,\n",
    "            gamma,\n",
    "            mu,\n",
    "            N,\n",
    "            n_runs=n_runs,\n",
    "            tmax=tmax,\n",
    "            num_points=num_points,\n",
    "            burn_frac=burn_frac,\n",
    "        )\n",
    "\n",
    "    vals = Parallel(n_jobs=n_jobs, verbose=10, prefer=\"processes\")(\n",
    "        delayed(run_point)(N, R0) for (N, R0) in params\n",
    "    )\n",
    "\n",
    "    M, B = len(N_grid), len(R0_grid)\n",
    "    return np.array(vals).reshape(M, B)\n",
    "\n",
    "\n",
    "gamma, mu = 0.1, 1 / 50\n",
    "N_grid = np.linspace(200, 3000, 10, dtype=int)\n",
    "R0_grid = np.linspace(1.5, 10, 10)\n",
    "\n",
    "mean_os = sweep_mean_overshoot(\n",
    "    N_grid,\n",
    "    R0_grid,\n",
    "    gamma,\n",
    "    mu,\n",
    "    n_runs=40,\n",
    "    tmax=800,\n",
    "    num_points=2000,\n",
    "    burn_frac=0.5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 5.5), constrained_layout=True)\n",
    "\n",
    "im = ax.imshow(\n",
    "    mean_os,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    extent=[R0_grid[0], R0_grid[-1], N_grid[0], N_grid[-1]],\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "ax.set_title(\"Mean peak overshoot (stationary window)\")\n",
    "ax.set_xlabel(r\"$R_0$\")\n",
    "ax.set_ylabel(\"Population size $N$\")\n",
    "c = plt.colorbar(im, ax=ax)\n",
    "c.set_label(\"Overshoot ratio\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, R0 = 100000, 4.0\n",
    "beta = R0 * (gamma + mu)\n",
    "eq = find_sir_equilibrium(beta, gamma, mu, N)\n",
    "print(f\"Equilibrium at N={N}, R0={R0}: {eq}\")\n",
    "print(f\"Y* = {eq[1]:.2f}, Y0 = {max(1, int(round(eq[1])))}\")\n",
    "\n",
    "# Run one trajectory and plot it\n",
    "rng = np.random.default_rng(2000)\n",
    "times, X_hist, Y_hist, Z_hist = gillespie_sir(\n",
    "    beta, gamma, mu, N, int(eq[0]), max(1, int(eq[1])), int(eq[2]), tmax=800, rng=rng\n",
    ")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Infected (count)\")\n",
    "plt.plot(times, Y_hist)\n",
    "plt.axhline(eq[1], color=\"red\", linestyle=\"--\", label=\"Deterministic equilibrium\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a range of R_0 values (> 1) by varying beta keeping gamma and mu fixed\n",
    "R0 = np.linspace(1, 2, 20)\n",
    "gamma = 0.1\n",
    "mu = 1 / 50\n",
    "beta = [R0_i * (gamma + mu) for R0_i in R0]\n",
    "N = 1000\n",
    "X0 = np.round(0.05 * N)\n",
    "Y0 = N - X0\n",
    "Z0 = 0\n",
    "seed0 = 42\n",
    "\n",
    "# Create an experiment that runs the gillespie simulation for the differnt R_0 values each n times for statistical significance. Per simulation count the number of extinctions, and the time to extiction. Take the averages over the runs and plot them against the values of R_0\n",
    "\n",
    "n_runs = 100  # Number of simulations per R0 value\n",
    "tmax = 500.0  # Maximum simulation time\n",
    "\n",
    "# Storage for results\n",
    "mean_extinction_counts = []\n",
    "mean_extinction_times = []\n",
    "\n",
    "\n",
    "for idx, (R0_val, beta_val) in enumerate(zip(R0, beta)):\n",
    "    extinctions = []\n",
    "    extinction_times = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        rng = np.random.default_rng(seed=seed0 + run)\n",
    "        times, X_hist, Y_hist, Z_hist = gillespie_sir(\n",
    "            beta_val, gamma, mu, N, X0, Y0, Z0, tmax=tmax, rng=rng\n",
    "        )\n",
    "\n",
    "        # Check if extinction occurred (Y reached 0 before tmax)\n",
    "        extinction_idx = np.where(Y_hist == 0)[0]\n",
    "\n",
    "        if len(extinction_idx) > 0:\n",
    "            extinctions.append(1)\n",
    "        else:\n",
    "            extinctions.append(0)\n",
    "\n",
    "    mean_extinction_counts.append(np.mean(extinctions))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(R0, mean_extinction_counts, linewidth=2)\n",
    "ax.set_xlabel(\"R_0\")\n",
    "ax.set_ylabel(\"Extinction Probability in 500 time units\")\n",
    "ax.set_title(f\"Extinction Probability vs R_0\\n(N={N}, n={n_runs} runs)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a range of R_0 values (> 1) by varying beta keeping gamma and mu fixed\n",
    "R0 = 1.3\n",
    "gamma = 0.1\n",
    "mu = 1 / 50\n",
    "beta = R0 * (gamma + mu)\n",
    "N = np.linspace(500, 6000, 20, dtype=int)\n",
    "X0 = 50\n",
    "Y0 = [N_val - X0 for N_val in N]\n",
    "Z0 = 0\n",
    "seed0 = 42\n",
    "\n",
    "# Create an experiment that runs the gillespie simulation for the differnt R_0 values each n times for statistical significance. Per simulation count the number of extinctions, and the time to extiction. Take the averages over the runs and plot them against the values of R_0\n",
    "\n",
    "n_runs = 100  # Number of simulations per R0 value\n",
    "tmax = 500.0  # Maximum simulation time\n",
    "\n",
    "# Storage for results\n",
    "mean_extinction_counts = []\n",
    "mean_extinction_times = []\n",
    "\n",
    "\n",
    "for idx, (N_val, Y0_val) in enumerate(zip(N, Y0)):\n",
    "    extinctions = []\n",
    "    extinction_times = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        rng = np.random.default_rng(seed=seed0 + run)\n",
    "        times, X_hist, Y_hist, Z_hist = gillespie_sir(\n",
    "            beta, gamma, mu, N_val, X0, Y0_val, Z0, tmax=tmax, rng=rng\n",
    "        )\n",
    "\n",
    "        # Check if extinction occurred (Y reached 0 before tmax)\n",
    "        extinction_idx = np.where(Y_hist == 0)[0]\n",
    "\n",
    "        if len(extinction_idx) > 0:\n",
    "            extinctions.append(1)\n",
    "        else:\n",
    "            extinctions.append(0)\n",
    "\n",
    "    mean_extinction_counts.append(np.mean(extinctions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(N, mean_extinction_counts, linewidth=2)\n",
    "ax.set_xlabel(\"N\")\n",
    "ax.set_ylabel(\"Extinction Probability\")\n",
    "ax.set_title(f\"Extinction Probability vs N\\n(R_0={R0}, n={n_runs} runs)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def run_parameter_combination(\n",
    "    R0_val, N_val, beta_val, gamma, mu, X0, tmax, n_runs, base_seed\n",
    "):\n",
    "    \"\"\"\n",
    "    Run simulations for a single (R0, N) parameter combination.\n",
    "    Returns extinction probability for that combination.\n",
    "    \"\"\"\n",
    "    extinctions = []\n",
    "    Y0_val = N_val - X0\n",
    "    Z0_val = 0\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        # Use unique seed for each run\n",
    "        seed = base_seed + run * 10000\n",
    "        rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        times, X_hist, Y_hist, Z_hist = gillespie_sir(\n",
    "            beta_val, gamma, mu, N_val, X0, Y0_val, Z0_val, tmax=tmax, rng=rng\n",
    "        )\n",
    "\n",
    "        # Check if extinction occurred (Y reached 0 before tmax)\n",
    "        extinction_idx = np.where(Y_hist == 0)[0]\n",
    "\n",
    "        if len(extinction_idx) > 0:\n",
    "            extinctions.append(1)\n",
    "        else:\n",
    "            extinctions.append(0)\n",
    "\n",
    "    return np.mean(extinctions)\n",
    "\n",
    "\n",
    "# Create heatmap of extinction probabilities varying N and R0 - JOBLIB VERSION\n",
    "gamma = 0.1\n",
    "mu = 1 / 50\n",
    "seed0 = 42\n",
    "\n",
    "# Define parameter ranges\n",
    "N_values = np.linspace(500, 5000, 10, dtype=int)  # Population sizes\n",
    "R0_values = np.linspace(1, 2, 10)  # R0 values\n",
    "beta_values = [R0_val * (gamma + mu) for R0_val in R0_values]\n",
    "\n",
    "# Fixed parameters\n",
    "X0 = 50  # Initial infected\n",
    "tmax = 500.0  # Maximum simulation time\n",
    "n_runs = 50  # Runs per parameter combination\n",
    "\n",
    "# Prepare parameter combinations for joblib\n",
    "param_combinations = []\n",
    "combo_id = 0\n",
    "\n",
    "for i, R0_val in enumerate(R0_values):\n",
    "    beta_val = beta_values[i]\n",
    "    for j, N_val in enumerate(N_values):\n",
    "        # Create unique base seed for each parameter combination\n",
    "        base_seed = seed0 + combo_id * 1000\n",
    "\n",
    "        param_combinations.append(\n",
    "            (i, j, R0_val, N_val, beta_val, gamma, mu, X0, tmax, n_runs, base_seed)\n",
    "        )\n",
    "        combo_id += 1\n",
    "\n",
    "# Run multiprocessing with joblib\n",
    "\n",
    "# Use joblib with all available cores and progress tracking\n",
    "results = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(run_parameter_combination)(\n",
    "        R0_val, N_val, beta_val, gamma, mu, X0, tmax, n_runs, base_seed\n",
    "    )\n",
    "    for i, j, R0_val, N_val, beta_val, gamma, mu, X0, tmax, n_runs, base_seed in param_combinations\n",
    ")\n",
    "\n",
    "# Reconstruct heatmap from results\n",
    "extinction_heatmap = np.zeros((len(R0_values), len(N_values)))\n",
    "\n",
    "for (i, j, *_), result in zip(param_combinations, results):\n",
    "    extinction_heatmap[i, j] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "im = ax.imshow(\n",
    "    extinction_heatmap, cmap=\"viridis\", aspect=\"auto\", origin=\"lower\", vmin=0, vmax=1\n",
    ")\n",
    "\n",
    "# Set ticks and labels\n",
    "n_ticks = 5\n",
    "r0_tick_indices = np.linspace(0, len(R0_values) - 1, n_ticks, dtype=int)\n",
    "n_tick_indices = np.linspace(0, len(N_values) - 1, n_ticks, dtype=int)\n",
    "\n",
    "ax.set_xticks(n_tick_indices)\n",
    "ax.set_xticklabels([f\"{N_values[i]:.0f}\" for i in n_tick_indices])\n",
    "ax.set_yticks(r0_tick_indices)\n",
    "ax.set_yticklabels([f\"{R0_values[i]:.1f}\" for i in r0_tick_indices])\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Population Size (N)\")\n",
    "ax.set_ylabel(\"Basic Reproduction Number (R0)\")\n",
    "ax.set_title(f\"Disease Extinction Probability Heatmap\\n(X0={X0})\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"Extinction Probability\")\n",
    "\n",
    "# Add contour lines for specific extinction probabilities\n",
    "contour_levels = [0.1, 0.5, 0.9]\n",
    "contours = ax.contour(\n",
    "    extinction_heatmap, levels=contour_levels, cmap=\"Wistia\", linewidths=2\n",
    ")\n",
    "ax.clabel(contours, inline=True, fontsize=15, fmt=\"%.1f\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndlib.models.ModelConfig as mc\n",
    "import ndlib.models.epidemics as ep\n",
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import networkx as nx\n",
    "from ndlib.viz.mpl.DiffusionTrend import DiffusionTrend\n",
    "from ndlib.viz.mpl.DiffusionPrevalence import DiffusionPrevalence\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSimulator:\n",
    "    def __init__(self, seed: int = 42):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.network_types: List[str] = [\n",
    "            \"erdos_renyi\",\n",
    "            \"barabasi_albert\",\n",
    "            \"watts_strogatz\",\n",
    "        ]\n",
    "        self.networks: Dict[str, List[nx.Graph]] = {}\n",
    "        self.experiment_results: Optional[pd.DataFrame] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_mean_degree(n_nodes: int, target_k: float) -> Tuple[float, int, int]:\n",
    "        if n_nodes <= 1:\n",
    "            p = 0.0\n",
    "        else:\n",
    "            p = target_k / (n_nodes - 1)\n",
    "        p = float(np.clip(p, 0.0, 1.0))\n",
    "\n",
    "        m = int(round(target_k / 2.0))\n",
    "        m = max(1, min(m, n_nodes - 1))\n",
    "\n",
    "        k = int(round(target_k))\n",
    "        k = min(k, n_nodes - 1)\n",
    "        if k % 2:\n",
    "            k = k - 1 if k > 1 else 2\n",
    "        k = max(2, min(k, n_nodes - 2))\n",
    "        return p, m, k\n",
    "    \n",
    "    \n",
    "    def add_network(self, name: str, G: nx.Graph):\n",
    "        \"\"\"Add a custom network to the simulator.\"\"\"\n",
    "        if name not in self.networks:\n",
    "            self.networks[name] = []\n",
    "            if name not in self.network_types:\n",
    "                self.network_types.append(name)\n",
    "        self.networks[name].append(G)\n",
    "        \n",
    "        \n",
    "    def load_sociopatterns(\n",
    "        self,\n",
    "        path: str,\n",
    "        threshold: float = 0.0,\n",
    "        giant_component: bool = True,\n",
    "        relabel_to_int: bool = True\n",
    "    )->nx.Graph:\n",
    "        \"\"\"Load the transmission_network.csv file\"\"\"\n",
    "        \n",
    "        df = pd.read_csv(path, sep=';', header=None)\n",
    "        \n",
    "        labels_row = df.iloc[0, 1:].astype(int).tolist()\n",
    "        labels_col = df.iloc[1:, 0].astype(int).tolist()\n",
    "        if labels_row != labels_col:\n",
    "            raise ValueError(\"Row/column labels mismatch in sociopatterns file.\")\n",
    "        \n",
    "        A = df.iloc[1:, 1:].to_numpy(dtype=float)\n",
    "        \n",
    "        \n",
    "        # Build graph, undirected & unweighted\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(labels_row)\n",
    "        n = A.shape[0]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if A[i, j] > threshold:\n",
    "                    G.add_edge(labels_row[i], labels_row[j])\n",
    "                    \n",
    "        if giant_component and G.number_of_nodes() > 0 and not nx.is_connected(G):\n",
    "             G = G.subgraph(max(nx.connected_components(G), key=len)).copy()\n",
    "             \n",
    "        \n",
    "        # relabel to integers 0..N-1\n",
    "\n",
    "        if relabel_to_int:\n",
    "            nx.set_node_attributes(G, {old: {\"orig_id\": old} for old in G.nodes()})\n",
    "            mapping = {old: i for i, old in enumerate(G.nodes())}\n",
    "            G = nx.relabel_nodes(G, mapping, copy=True)\n",
    "                    \n",
    "        # register\n",
    "        self.add_network(\"sociopatterns\", G)\n",
    "        \n",
    "        # Sanity check\n",
    "        degs = [d for _, d in G.degree()]\n",
    "        print(f\"[Sociopatterns] N={G.number_of_nodes()}, L={G.number_of_edges()}, ⟨k⟩={np.mean(degs):.2f}\")\n",
    "        return G\n",
    "\n",
    "\n",
    "    # Network generation\n",
    "    def generate_networks(\n",
    "        self,\n",
    "        n_nodes: int = 100,\n",
    "        n_instances: int = 5,\n",
    "        target_k: float = 6.0,\n",
    "        ws_p: float = 0.3,\n",
    "    ) -> Dict[str, List[nx.Graph]]:\n",
    "        p, m, k = self._set_mean_degree(n_nodes, target_k)\n",
    "        print(\n",
    "            f\"[Generate] N={n_nodes}, instances={n_instances}, target ⟨k⟩={target_k:.2f}\"\n",
    "        )\n",
    "        print(f\"  ER: p={p:.4f}   BA: m={m}   WS: k={k}, p_rewire={ws_p}\")\n",
    "\n",
    "        self.networks = {t: [] for t in self.network_types}\n",
    "        for i in range(n_instances):\n",
    "            base = self.seed + i\n",
    "            self.networks[\"erdos_renyi\"].append(\n",
    "                nx.erdos_renyi_graph(n_nodes, p, seed=base)\n",
    "            )\n",
    "            self.networks[\"barabasi_albert\"].append(\n",
    "                nx.barabasi_albert_graph(n_nodes, m, seed=base)\n",
    "            )\n",
    "            self.networks[\"watts_strogatz\"].append(\n",
    "                nx.watts_strogatz_graph(n_nodes, k, ws_p, seed=base)\n",
    "            )\n",
    "\n",
    "        for t in self.network_types:\n",
    "            means = []\n",
    "            for G in self.networks[t]:\n",
    "                degs = [d for _, d in G.degree()]\n",
    "                means.append(np.mean(degs))\n",
    "            print(\n",
    "                f\"  {t.replace('_',' ').title():>16}: realized ⟨k⟩ = {np.mean(means):.2f} ± {np.std(means):.2f}\"\n",
    "            )\n",
    "        return self.networks\n",
    "\n",
    "    # Network metrics (structure + epidemic-relevant)\n",
    "\n",
    "    def compute_network_stats(self, G: nx.Graph) -> Dict[str, float]:\n",
    "        \"\"\"Structure metrics + epidemic-relevant degree moments (⟨k⟩, ⟨k²⟩, τc).\"\"\"\n",
    "        stats: Dict[str, float] = {}\n",
    "        n = G.number_of_nodes()\n",
    "        m = G.number_of_edges()\n",
    "        stats[\"n_nodes\"] = n\n",
    "        stats[\"n_edges\"] = m\n",
    "        stats[\"density\"] = nx.density(G)\n",
    "\n",
    "        # Degrees & moments\n",
    "        degs = (\n",
    "            np.fromiter((d for _, d in G.degree()), dtype=float, count=n)\n",
    "            if n\n",
    "            else np.array([])\n",
    "        )\n",
    "        if n > 0 and degs.size > 0:\n",
    "            k1 = float(degs.mean())\n",
    "            k2 = float((degs**2).mean())\n",
    "            stats[\"avg_degree\"] = k1\n",
    "            stats[\"std_degree\"] = float(degs.std())\n",
    "            stats[\"min_degree\"] = float(degs.min())\n",
    "            stats[\"max_degree\"] = float(degs.max())\n",
    "            stats[\"second_moment_k2\"] = k2\n",
    "            stats[\"degree_heterogeneity\"] = float(degs.var() / k1) if k1 > 0 else np.nan\n",
    "            stats[\"tau_c_mean_over_second\"] = (\n",
    "                (k1 / k2) if k2 > 0 else np.nan\n",
    "            )  # threshold proxy\n",
    "        else:\n",
    "            stats.update(\n",
    "                {\n",
    "                    \"avg_degree\": 0.0,\n",
    "                    \"std_degree\": 0.0,\n",
    "                    \"min_degree\": 0.0,\n",
    "                    \"max_degree\": 0.0,\n",
    "                    \"second_moment_k2\": 0.0,\n",
    "                    \"degree_heterogeneity\": np.nan,\n",
    "                    \"tau_c_mean_over_second\": np.nan,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if n == 0:\n",
    "            stats.update(\n",
    "                {\n",
    "                    \"n_components\": 0,\n",
    "                    \"lcc_frac\": 0.0,\n",
    "                    \"avg_path_length\": np.nan,\n",
    "                    \"diameter\": np.nan,\n",
    "                    \"avg_clustering\": np.nan,\n",
    "                    \"transitivity\": np.nan,\n",
    "                    \"second_moment_k2\": 0.0,\n",
    "                    \"degree_heterogeneity\": np.nan,\n",
    "                    \"tau_c_mean_over_second\": np.nan,\n",
    "                    \"avg_betweenness\": np.nan,\n",
    "                }\n",
    "            )\n",
    "            return stats\n",
    "\n",
    "        # Components/LCC\n",
    "        if nx.is_connected(G):\n",
    "            H = G\n",
    "            stats[\"n_components\"] = 1\n",
    "            stats[\"lcc_frac\"] = 1.0\n",
    "        else:\n",
    "            components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "            H = G.subgraph(components[0]).copy()\n",
    "            stats[\"n_components\"] = nx.number_connected_components(G)\n",
    "            stats[\"lcc_frac\"] = len(H) / n\n",
    "\n",
    "        # Distances on LCC\n",
    "        if H.number_of_nodes() > 1:\n",
    "            stats[\"avg_path_length\"] = nx.average_shortest_path_length(H)\n",
    "            stats[\"diameter\"] = nx.diameter(H)\n",
    "        else:\n",
    "            stats[\"avg_path_length\"] = 0.0\n",
    "            stats[\"diameter\"] = 0.0\n",
    "\n",
    "        # Clustering (local mean) & global transitivity\n",
    "        stats[\"avg_clustering\"] = nx.average_clustering(G) if n > 0 else np.nan\n",
    "        stats[\"transitivity\"] = nx.transitivity(G) if n > 0 else np.nan\n",
    "\n",
    "        # Approximate betweenness \n",
    "        k_sample = min(100, n)\n",
    "        bet = nx.betweenness_centrality(G, k=k_sample, seed=self.seed)\n",
    "        stats[\"avg_betweenness\"] = float(np.mean(list(bet.values()))) if bet else 0.0\n",
    "        return stats\n",
    "\n",
    "\n",
    "    # Epidemic metrics (computed from NDlib trends)\n",
    "\n",
    "    def compute_epidemic_metrics(self, result: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Standard epidemic metrics from a run_sir_simulation result.\n",
    "        - attack_rate: final recovered / N\n",
    "        - epidemic_duration: last time any I(t) > 0 (iterations)\n",
    "        - exponential_growth_rate: slope of log(I+1) in early phase\n",
    "        \"\"\"\n",
    "        N = result[\"n_nodes\"]\n",
    "        trends = result[\"trends\"][0][\"trends\"][\"node_count\"]  # NDlib structure\n",
    "        I_curve = np.asarray(trends[1], dtype=float)  # 1 = Infected\n",
    "        R_curve = np.asarray(trends[2], dtype=float)  # 2 = Removed\n",
    "\n",
    "        attack_rate = float(R_curve[-1] / N) if N > 0 else np.nan\n",
    "\n",
    "        nz = np.nonzero(I_curve)[0]\n",
    "        epidemic_duration = int(nz[-1] + 1) if nz.size > 0 else 0\n",
    "\n",
    "        t_peak = int(result[\"time_to_peak\"])\n",
    "        cutoff = max(3, int(0.3 * t_peak))  # up to ~30% of peak time\n",
    "        early = I_curve[:cutoff]\n",
    "        if early.size >= 3:\n",
    "            logI = np.log(early + 1.0)  # avoid log(0)\n",
    "            x = np.arange(len(logI))\n",
    "            slope, _ = np.polyfit(x, logI, 1)\n",
    "            growth_rate = float(slope)\n",
    "        else:\n",
    "            growth_rate = np.nan\n",
    "\n",
    "        return {\n",
    "            \"attack_rate\": attack_rate,\n",
    "            \"epidemic_duration\": epidemic_duration,\n",
    "            \"exponential_growth_rate\": growth_rate,\n",
    "        }\n",
    "\n",
    "    # Merge network + epidemic metrics in one dict\n",
    "\n",
    "    def compute_all_metrics(self, G: nx.Graph, sim_result: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        One-stop metrics aggregator:\n",
    "        - Network stats (structure + ⟨k⟩, ⟨k²⟩, τc)\n",
    "        - Epidemic summaries (peak, timing, final size, attack rate, duration, growth rate)\n",
    "        - Basic R0 (β/γ)\n",
    "        \"\"\"\n",
    "        net = self.compute_network_stats(G)\n",
    "        epi = self.compute_epidemic_metrics(sim_result)\n",
    "        merged = dict(net)\n",
    "        merged.update(\n",
    "            {\n",
    "                \"beta\": sim_result[\"beta\"],\n",
    "                \"gamma\": sim_result[\"gamma\"],\n",
    "                \"R0_naive\": (\n",
    "                    (sim_result[\"beta\"] / sim_result[\"gamma\"])\n",
    "                    if sim_result[\"gamma\"] > 0\n",
    "                    else np.inf\n",
    "                ),\n",
    "                \"peak_infected\": sim_result[\"peak_infected\"],\n",
    "                \"time_to_peak\": sim_result[\"time_to_peak\"],\n",
    "                \"final_recovered\": sim_result[\"final_recovered\"],\n",
    "                \"peak_frac\": sim_result[\"peak_frac\"],\n",
    "                \"final_size_frac\": sim_result[\"final_size_frac\"],\n",
    "            }\n",
    "        )\n",
    "        merged.update(epi)\n",
    "        return merged\n",
    "\n",
    "\n",
    "    # SIR simulations\n",
    "\n",
    "    def run_sir_simulation(\n",
    "        self,\n",
    "        G: nx.Graph,\n",
    "        beta: float = 0.5,\n",
    "        gamma: float = 0.1,\n",
    "        fraction_infected: float = 0.05,\n",
    "        iterations: int = 200,\n",
    "        seed: Optional[int] = None,\n",
    "    ) -> Dict[str, object]:\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        model = ep.SIRModel(G)\n",
    "        config = mc.Configuration()\n",
    "        config.add_model_parameter(\"beta\", float(beta))\n",
    "        config.add_model_parameter(\"gamma\", float(gamma))\n",
    "        config.add_model_parameter(\"fraction_infected\", float(fraction_infected))\n",
    "        model.set_initial_status(config)\n",
    "\n",
    "        iters = model.iteration_bunch(iterations)\n",
    "        trends = model.build_trends(iters)\n",
    "\n",
    "        s2i = getattr(\n",
    "            model, \"available_statuses\", {\"Susceptible\": 0, \"Infected\": 1, \"Removed\": 2}\n",
    "        )\n",
    "        I_idx = s2i[\"Infected\"]\n",
    "        R_idx = s2i[\"Removed\"]\n",
    "\n",
    "        node_count = trends[0][\"trends\"][\"node_count\"]\n",
    "        infected = np.asarray(node_count[I_idx], dtype=float)\n",
    "        removed = np.asarray(node_count[R_idx], dtype=float)\n",
    "\n",
    "        peak_infected = float(infected.max())\n",
    "        time_to_peak = int(infected.argmax())\n",
    "        final_recovered = float(removed[-1])\n",
    "        N = G.number_of_nodes()\n",
    "\n",
    "        return {\n",
    "            \"trends\": trends,\n",
    "            \"iterations\": iters,\n",
    "            \"beta\": beta,\n",
    "            \"gamma\": gamma,\n",
    "            \"fraction_infected\": fraction_infected,\n",
    "            \"n_nodes\": N,\n",
    "            \"peak_infected\": peak_infected,\n",
    "            \"time_to_peak\": time_to_peak,\n",
    "            \"final_recovered\": final_recovered,\n",
    "            \"peak_frac\": peak_infected / N if N else np.nan,\n",
    "            \"final_size_frac\": final_recovered / N if N else np.nan,\n",
    "        }\n",
    "\n",
    "\n",
    "    # Sweeps\n",
    "\n",
    "    def experiment_parameter_sweep(\n",
    "        self,\n",
    "        n_repetitions: int = 5,\n",
    "        beta_values: Optional[List[float]] = None,\n",
    "        gamma_values: Optional[List[float]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not self.networks:\n",
    "            raise RuntimeError(\"No networks stored\")\n",
    "\n",
    "        if beta_values is None:\n",
    "            beta_values = [0.05, 0.1, 0.2]\n",
    "        if gamma_values is None:\n",
    "            gamma_values = [0.02, 0.05, 0.1]\n",
    "\n",
    "        rows = []\n",
    "        for network_type in self.network_types:\n",
    "            nets = self.networks[network_type]\n",
    "            for beta in beta_values:\n",
    "                for gamma in gamma_values: \n",
    "                    for r in range(n_repetitions):\n",
    "                        idx = int(self.rng.integers(0, len(nets)))\n",
    "                        G = nets[idx]\n",
    "                        res = self.run_sir_simulation(\n",
    "                            G,\n",
    "                            beta=beta,\n",
    "                            gamma=gamma,\n",
    "                            fraction_infected=0.05,\n",
    "                            iterations=200,\n",
    "                            seed=self.seed + r,\n",
    "                        )\n",
    "                        allm = self.compute_all_metrics(G, res)\n",
    "                        allm.update(\n",
    "                            {\n",
    "                                \"network_type\": network_type,\n",
    "                                \"rep\": r,\n",
    "                                \"net_idx\": idx,\n",
    "                            }\n",
    "                        )\n",
    "                        rows.append(allm)\n",
    "        self.experiment_results = pd.DataFrame(rows)\n",
    "        return self.experiment_results\n",
    "\n",
    "    # Seeding strategies (random/high/low degree)\n",
    "\n",
    "    def experiment_initial_conditions(\n",
    "        self,\n",
    "        beta: float = 0.5,\n",
    "        gamma: float = 0.1,\n",
    "        iterations: int = 200,\n",
    "        fractions: List[float] = [0.01, 0.05, 0.1],\n",
    "        repetitions: int = 5,\n",
    "    ) -> pd.DataFrame:\n",
    "        if not self.networks:\n",
    "            raise RuntimeError(\"No networks stored\")\n",
    "\n",
    "        strategies = {\n",
    "            \"random\": self._infect_random,\n",
    "            \"high_degree\": self._infect_high_degree,\n",
    "            \"low_degree\": self._infect_low_degree,\n",
    "        }\n",
    "\n",
    "        rows = []\n",
    "        for net_type in self.network_types:\n",
    "            for frac in fractions:\n",
    "                for strat_name, strat_fn in strategies.items():\n",
    "                    for rep in range(repetitions):\n",
    "                        G = self.networks[net_type][rep % len(self.networks[net_type])]\n",
    "\n",
    "                        model = ep.SIRModel(G)\n",
    "                        cfg = mc.Configuration()\n",
    "                        cfg.add_model_parameter(\"beta\", float(beta))\n",
    "                        cfg.add_model_parameter(\"gamma\", float(gamma))\n",
    "\n",
    "                        infected0 = strat_fn(G, frac)\n",
    "                        for u in G.nodes():\n",
    "                            cfg.add_node_configuration(\n",
    "                                \"status\", u, 1 if u in infected0 else 0\n",
    "                            )\n",
    "\n",
    "                        model.set_initial_status(cfg)\n",
    "                        its = model.iteration_bunch(iterations)\n",
    "                        trends = model.build_trends(its)\n",
    "\n",
    "                        s2i = getattr(\n",
    "                            model,\n",
    "                            \"available_statuses\",\n",
    "                            {\"Susceptible\": 0, \"Infected\": 1, \"Removed\": 2},\n",
    "                        )\n",
    "                        I_idx, R_idx = s2i[\"Infected\"], s2i[\"Removed\"]\n",
    "                        node_count = trends[0][\"trends\"][\"node_count\"]\n",
    "                        infected = np.asarray(node_count[I_idx], dtype=float)\n",
    "                        removed = np.asarray(node_count[R_idx], dtype=float)\n",
    "\n",
    "                        peak = float(infected.max())\n",
    "                        tpeak = int(infected.argmax())\n",
    "                        finalR = float(removed[-1])\n",
    "                        N = G.number_of_nodes()\n",
    "\n",
    "                        res = {\n",
    "                            \"trends\": trends,\n",
    "                            \"n_nodes\": N,\n",
    "                            \"beta\": beta,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"time_to_peak\": tpeak,\n",
    "                            \"peak_infected\": peak,\n",
    "                            \"final_recovered\": finalR,\n",
    "                            \"peak_frac\": peak / N,\n",
    "                            \"final_size_frac\": finalR / N,\n",
    "                        }\n",
    "                        allm = self.compute_all_metrics(G, res)\n",
    "                        allm.update(\n",
    "                            {\n",
    "                                \"network_type\": net_type,\n",
    "                                \"strategy\": strat_name,\n",
    "                                \"seed_fraction\": frac,\n",
    "                                \"rep\": rep,\n",
    "                            }\n",
    "                        )\n",
    "                        rows.append(allm)\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Seeding helpers\n",
    "\n",
    "    def _infect_random(self, G: nx.Graph, fraction: float) -> set:\n",
    "        n = G.number_of_nodes()\n",
    "        k = max(1, int(round(fraction * n)))\n",
    "        nodes = np.array(G.nodes())\n",
    "        return set(self.rng.choice(nodes, size=k, replace=False))\n",
    "\n",
    "    def _infect_high_degree(self, G: nx.Graph, fraction: float) -> set:\n",
    "        n = G.number_of_nodes()\n",
    "        k = max(1, int(round(fraction * n)))\n",
    "        sorted_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
    "        return set([node for node, _ in sorted_nodes[:k]])\n",
    "\n",
    "    def _infect_low_degree(self, G: nx.Graph, fraction: float) -> set:\n",
    "        n = G.number_of_nodes()\n",
    "        k = max(1, int(round(fraction * n)))\n",
    "        return set([u for u, _d in sorted(G.degree(), key=lambda x: x[1])[:k]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################## Vaccination Helpers ##########################\n",
    "    \n",
    "    def _ndlib_init_with_seeds(\n",
    "    self, G: nx.Graph, beta: float, gamma: float,\n",
    "    infected0: List[int], removed0: Optional[List[int]] = None\n",
    "    ):\n",
    "        \"\"\"Create an NDlib SIR model with explicit initial infected/removed nodes.\"\"\"\n",
    "        removed0 = removed0 or []\n",
    "        model = ep.SIRModel(G)\n",
    "        config = mc.Configuration()\n",
    "        config.add_model_parameter(\"beta\", float(beta))\n",
    "        config.add_model_parameter(\"gamma\", float(gamma))\n",
    "        # everyone S=0 by default\n",
    "        for u in G.nodes():\n",
    "            config.add_node_configuration(\"status\", u, 0)\n",
    "        for u in infected0:\n",
    "            config.add_node_configuration(\"status\", u, 1)  \n",
    "        for u in removed0:\n",
    "            config.add_node_configuration(\"status\", u, 2) \n",
    "        model.set_initial_status(config)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def _test_node(self, true_status: int, acc: float, rng: np.random.Generator) -> int:\n",
    "        \"\"\"\n",
    "        Return observed status in {0:S, 1:I}. With prob=acc it's correct, else flipped.\n",
    "        \"\"\"\n",
    "        true = 0 if true_status == 2 else true_status  # map R->S for observation channel\n",
    "        flip = (rng.random() > acc)\n",
    "        return (1 - true) if flip else true\n",
    "\n",
    "\n",
    "    def score_nodes(\n",
    "        self, G: nx.Graph, obs_status: Dict[int, int],\n",
    "        alpha: float = 1.0, beta_w: float = 0.25, gamma_w: float = 0.0\n",
    "    ) -> Dict[int, float]:\n",
    "        \"\"\"\n",
    "        score(u) = alpha  * (# observed-infected neighbors of u)\n",
    "                + beta_w * degree(u)\n",
    "                + gamma_w* (# 2-hop paths from u to observed infecteds)\n",
    "        obs_status: dict u -> {UNK:-1, S:0, I:1, R:2}\n",
    "        \"\"\"\n",
    "        score: Dict[int, float] = {}\n",
    "        I_obs = {u for u, st in obs_status.items() if st == 1}\n",
    "        for u in G.nodes():\n",
    "            if obs_status.get(u, -1) == 2:\n",
    "                continue  # skip observed R\n",
    "            nI = sum(1 for v in G.neighbors(u) if obs_status.get(v, -1) == 1)\n",
    "            k = G.degree(u)\n",
    "            if gamma_w > 0.0 and I_obs:\n",
    "                h2 = 0\n",
    "                for v in G.neighbors(u):\n",
    "                    for w in G.neighbors(v):\n",
    "                        if w != u and obs_status.get(w, -1) == 1:\n",
    "                            h2 += 1\n",
    "            else:\n",
    "                h2 = 0\n",
    "            score[u] = alpha * nI + beta_w * k + gamma_w * h2\n",
    "        return score\n",
    "\n",
    "    def experiment_vaccination_mixed(\n",
    "        self,\n",
    "        G: nx.Graph,\n",
    "        beta: float,\n",
    "        gamma: float,\n",
    "        tests_per_step: int,\n",
    "        vaccines_per_step: int,\n",
    "        test_accuracy: float,\n",
    "        iterations: int = 300,\n",
    "        seed_inits: int = 5,      # initial infected count\n",
    "        alpha_w: float = 1.0,     # risk score weights\n",
    "        beta_w: float = 0.25,\n",
    "        gamma_w: float = 0.0,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dynamic Ring + Acquaintance Vax\n",
    "        Returns time series + summary metrics.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        nodes = np.array(list(G.nodes()))\n",
    "        infected0 = list(rng.choice(nodes, size=min(seed_inits, len(nodes)), replace=False))\n",
    "        removed0: List[int] = []\n",
    "\n",
    "        # NDlib model (true state)\n",
    "        model = self._ndlib_init_with_seeds(G, beta, gamma, infected0, removed0)\n",
    "\n",
    "        # -1=UNK, 0=S, 1=I, 2=R (start unknown; learn via testing)\n",
    "        obs_status: Dict[int, int] = {u: -1 for u in G.nodes()}\n",
    "\n",
    "        # Logs & caches\n",
    "        I_curve: List[int] = []\n",
    "        R_curve: List[int] = []\n",
    "        SIR_counts: List[Tuple[int, int, int]] = []\n",
    "        peak_I, t_peak = 0.0, 0\n",
    "        N = G.number_of_nodes()\n",
    "        deg_dict = dict(G.degree())\n",
    "\n",
    "        for t in range(iterations):\n",
    "            true_status = model.status  # {node: 0/1/2}\n",
    "\n",
    "            # Test selection \n",
    "            scores = self.score_nodes(G, obs_status, alpha=alpha_w, beta_w=beta_w, gamma_w=gamma_w)\n",
    "            candidates = [(u, s) for u, s in scores.items() if obs_status.get(u, -1) != 2]\n",
    "            candidates.sort(key=lambda x: (x[1], deg_dict[x[0]]), reverse=True)\n",
    "            to_test = [u for u, _ in candidates[:max(0, tests_per_step)]]\n",
    "\n",
    "            for u in to_test:\n",
    "                lab = self._test_node(true_status[u], test_accuracy, rng)  # 0 or 1\n",
    "                obs_status[u] = lab\n",
    "                \n",
    "            # After testing, before vaccination\n",
    "            if t == 50:  # check at one snapshot\n",
    "                false_positives = sum(1 for u in obs_status if obs_status[u] == 1 and model.status[u] != 1)\n",
    "                true_positives = sum(1 for u in obs_status if obs_status[u] == 1 and model.status[u] == 1)\n",
    "                print(f\"False positives: {false_positives}, True positives: {true_positives}\")\n",
    "\n",
    "            # Vaccination step\n",
    "            vaccines_left = vaccines_per_step\n",
    "\n",
    "            # Ring vaccination: neighbors of observed infected (unknown OR tested-S)\n",
    "            I_obs = {u for u, st in obs_status.items() if st == 1}\n",
    "            ring_candidates: Set[int] = set()\n",
    "            if I_obs:\n",
    "                for w in I_obs:\n",
    "                    for v in G.neighbors(w):\n",
    "                        if obs_status.get(v, -1) in (-1, 0):  # unknown OR tested as S\n",
    "                            ring_candidates.add(v)\n",
    "\n",
    "            ring_sorted = sorted(ring_candidates, key=lambda u: deg_dict.get(u, 0), reverse=True)\n",
    "            for v in ring_sorted:\n",
    "                if vaccines_left == 0:\n",
    "                    break\n",
    "                if model.status[v] != 2:\n",
    "                    model.status[v] = 2\n",
    "                    obs_status[v] = 2\n",
    "                    vaccines_left -= 1\n",
    "\n",
    "            # random neighbor of random non-R node\n",
    "            tries = 0\n",
    "            while vaccines_left > 0 and tries < 5 * N:\n",
    "                x = int(rng.choice(nodes))\n",
    "                if obs_status.get(x, -1) == 2:\n",
    "                    tries += 1\n",
    "                    continue\n",
    "                neigh = [y for y in G.neighbors(x) if model.status[y] != 2]\n",
    "                if not neigh:\n",
    "                    tries += 1\n",
    "                    continue\n",
    "                y = int(rng.choice(neigh))\n",
    "                if model.status[y] != 2:\n",
    "                    model.status[y] = 2\n",
    "                    obs_status[y] = 2\n",
    "                    vaccines_left -= 1\n",
    "                tries += 1\n",
    "\n",
    "            # advance epidemic\n",
    "            _ = model.iteration()\n",
    "            vals = np.array([model.status[u] for u in G.nodes()], dtype=int)\n",
    "            S = int(np.sum(vals == 0)); I = int(np.sum(vals == 1)); R = int(np.sum(vals == 2))\n",
    "            SIR_counts.append((S, I, R))\n",
    "            I_curve.append(I); R_curve.append(R)\n",
    "\n",
    "            if I > peak_I:\n",
    "                peak_I, t_peak = I, t\n",
    "            if I == 0:\n",
    "                break\n",
    "\n",
    "        peak_frac = (peak_I / N) if N else np.nan\n",
    "        final_size_frac = (R_curve[-1] / N) if R_curve else 0.0\n",
    "        duration = len(I_curve)\n",
    "\n",
    "        return {\n",
    "            \"policy\": \"RPA\",\n",
    "            \"beta\": beta, \"gamma\": gamma,\n",
    "            \"tests_per_step\": tests_per_step,\n",
    "            \"vaccines_per_step\": vaccines_per_step,\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"iterations\": duration,\n",
    "            \"SIR_counts\": np.array(SIR_counts, dtype=int),\n",
    "            \"peak_infected\": float(peak_I),\n",
    "            \"time_to_peak\": int(t_peak),\n",
    "            \"final_recovered\": float(R_curve[-1]) if R_curve else 0.0,\n",
    "            \"peak_frac\": peak_frac,\n",
    "            \"final_size_frac\": final_size_frac,\n",
    "            \"n_nodes\": N,\n",
    "        }\n",
    "\n",
    "\n",
    "    ########################## Random Vaccination ##########################\n",
    "\n",
    "    def experiment_vaccination_random(\n",
    "        self,\n",
    "        G: nx.Graph,\n",
    "        beta: float,\n",
    "        gamma: float,\n",
    "        tests_per_step: int,     # kept for symmetry with mixed\n",
    "        vaccines_per_step: int,\n",
    "        test_accuracy: float,    # kept for symmetry\n",
    "        iterations: int = 300,\n",
    "        seed_inits: int = 5,\n",
    "        seed: int = 5678,\n",
    "    ):\n",
    "        \"\"\" Vaccinate V random non-Removed nodes each step; no testing logic used.\"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        nodes = np.array(list(G.nodes()))\n",
    "        infected0 = list(rng.choice(nodes, size=min(seed_inits, len(nodes)), replace=False))\n",
    "        model = self._ndlib_init_with_seeds(G, beta, gamma, infected0, [])\n",
    "\n",
    "        I_curve: List[int] = []\n",
    "        R_curve: List[int] = []\n",
    "        SIR_counts: List[Tuple[int, int, int]] = []\n",
    "        peak_I, t_peak = 0.0, 0\n",
    "        N = G.number_of_nodes()\n",
    "\n",
    "        for t in range(iterations):\n",
    "            # vaccinate up to V random nodes\n",
    "            eligible = [u for u in G.nodes() if model.status[u] != 2]\n",
    "            rng.shuffle(eligible)\n",
    "            for v in eligible[:max(0, vaccines_per_step)]:\n",
    "                model.status[v] = 2\n",
    "\n",
    "            _ = model.iteration()\n",
    "            vals = np.array([model.status[u] for u in G.nodes()], dtype=int)\n",
    "            S = int(np.sum(vals == 0)); I = int(np.sum(vals == 1)); R = int(np.sum(vals == 2))\n",
    "            SIR_counts.append((S, I, R))\n",
    "            I_curve.append(I); R_curve.append(R)\n",
    "            if I > peak_I:\n",
    "                peak_I, t_peak = I, t\n",
    "            if I == 0:\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            \"policy\": \"random\",\n",
    "            \"beta\": beta, \"gamma\": gamma,\n",
    "            \"tests_per_step\": tests_per_step,\n",
    "            \"vaccines_per_step\": vaccines_per_step,\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"iterations\": len(I_curve),\n",
    "            \"SIR_counts\": np.array(SIR_counts, dtype=int),\n",
    "            \"peak_infected\": float(peak_I),\n",
    "            \"time_to_peak\": int(t_peak),\n",
    "            \"final_recovered\": float(R_curve[-1]) if R_curve else 0.0,\n",
    "            \"peak_frac\": (peak_I / N) if N else np.nan,\n",
    "            \"final_size_frac\": (R_curve[-1] / N) if R_curve else 0.0,\n",
    "            \"n_nodes\": N,\n",
    "        }\n",
    "\n",
    "\n",
    "    ########################## Sweep ##########################\n",
    "\n",
    "    def sweep_dynamic_vaccination(\n",
    "        self, G: nx.Graph, beta: float, gamma: float,\n",
    "        budgets=(1, 3, 5, 10), accs=(0.5, 0.75, 1.0),\n",
    "        iterations: int = 300, reps: int = 30, seed0: int = 2025\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Runs RPA and Random policies over budgets/accuracies with 'reps' replicates.\n",
    "        Returns a DataFrame with summary metrics.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for b in budgets:\n",
    "            v = b  # equal split tests/vaccines\n",
    "            for acc in accs:\n",
    "                for r in range(reps):\n",
    "                    s = seed0 + 100 * r\n",
    "                    mixed = self.experiment_vaccination_mixed(\n",
    "                        G, beta, gamma, tests_per_step=b, vaccines_per_step=v,\n",
    "                        test_accuracy=acc, iterations=iterations, seed=s\n",
    "                    )\n",
    "                    rnd = self.experiment_vaccination_random(\n",
    "                        G, beta, gamma, tests_per_step=b, vaccines_per_step=v,\n",
    "                        test_accuracy=acc, iterations=iterations, seed=s + 1\n",
    "                    )\n",
    "                    for res in (mixed, rnd):\n",
    "                        rows.append({\n",
    "                            \"policy\": res[\"policy\"],\n",
    "                            \"tests_per_step\": res[\"tests_per_step\"],\n",
    "                            \"vaccines_per_step\": res[\"vaccines_per_step\"],\n",
    "                            \"test_accuracy\": res[\"test_accuracy\"],\n",
    "                            \"rep\": r,\n",
    "                            \"peak_frac\": res[\"peak_frac\"],\n",
    "                            \"final_size_frac\": res[\"final_size_frac\"],\n",
    "                            \"time_to_peak\": res[\"time_to_peak\"],\n",
    "                            \"duration\": res[\"iterations\"],\n",
    "                            \"n_nodes\": res[\"n_nodes\"],\n",
    "                            \"beta\": res[\"beta\"],\n",
    "                            \"gamma\": res[\"gamma\"],\n",
    "                            \"R0\": (beta / gamma) if gamma > 0 else np.inf,\n",
    "                        })\n",
    "        return pd.DataFrame(rows)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(\n",
    "    network_n=400, instances=5, target_k=8, ws_p=0.15, beta=0.10, gamma=0.05, iters=220\n",
    "):\n",
    "    sim = NetworkSimulator(seed=123)\n",
    "\n",
    "    # Build networks\n",
    "    sim.generate_networks(\n",
    "        n_nodes=network_n, n_instances=instances, target_k=target_k, ws_p=ws_p\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for nt in sim.network_types:\n",
    "        stats = [sim.compute_network_stats(G) for G in sim.networks[nt]]\n",
    "        df = pd.DataFrame(stats)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"network_type\": nt,\n",
    "                \"N_mean±sd\": f\"{df['n_nodes'].mean():.0f}±{df['n_nodes'].std():.0f}\",\n",
    "                \"⟨k⟩ mean±sd\": f\"{df['avg_degree'].mean():.2f}±{df['avg_degree'].std():.2f}\",\n",
    "                \"std(k) mean\": f\"{df['std_degree'].mean():.2f}\",\n",
    "                \"clustering mean\": f\"{df['avg_clustering'].mean():.3f}\",\n",
    "                \"LCC frac mean\": f\"{df['lcc_frac'].mean():.3f}\",\n",
    "                \"τc=<k>/<k²> mean\": f\"{df['tau_c_mean_over_second'].mean():.4f}\",\n",
    "                \"APL(LCC) mean\": f\"{df['avg_path_length'].mean():.2f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\\n Averaged metrics per network type\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "\n",
    "    # SIR simulation per network, with metrics\n",
    "    epi_rows = []\n",
    "    for nt in sim.network_types:\n",
    "        G = sim.networks[nt][0]\n",
    "        res = sim.run_sir_simulation(\n",
    "            G, beta=beta, gamma=gamma, fraction_infected=0.1, iterations=iters, seed=123\n",
    "        )\n",
    "        allm = sim.compute_all_metrics(G, res)\n",
    "        epi_rows.append(\n",
    "            {\n",
    "                \"network_type\": nt,\n",
    "                \"R0=β/γ\": f\"{allm['R0_naive']:.2f}\",\n",
    "                \"peak_frac\": f\"{allm['peak_frac']:.3f}\",\n",
    "                \"time_to_peak\": int(allm[\"time_to_peak\"]),\n",
    "                \"final_size_frac\": f\"{allm['final_size_frac']:.3f}\",\n",
    "                \"attack_rate\": f\"{allm['attack_rate']:.3f}\",\n",
    "                \"duration\": int(allm[\"epidemic_duration\"]),\n",
    "                \"growth_rate(early)\": f\"{allm['exponential_growth_rate']:.3f}\",\n",
    "            }\n",
    "        )\n",
    "    print(\"\\n Key outcomes (first instance of each type)\")\n",
    "    print(pd.DataFrame(epi_rows).to_string(index=False))\n",
    "\n",
    "    betas = [0.05, 0.08, 0.12, 0.18]\n",
    "    gammas = [0.03, 0.05, 0.08]\n",
    "    df = sim.experiment_parameter_sweep(\n",
    "        n_repetitions=3, beta_values=betas, gamma_values=gammas\n",
    "    )\n",
    "\n",
    "    def tidy(group):\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"peak_frac_mean\": group[\"peak_frac\"].mean(),\n",
    "                \"final_size_mean\": group[\"final_size_frac\"].mean(),\n",
    "                \"time_to_peak_mean\": group[\"time_to_peak\"].mean(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    summary = df.groupby([\"network_type\", \"beta\", \"gamma\"]).apply(tidy).reset_index()\n",
    "\n",
    "    print(\"\\nSummary (means over reps/instances)\")\n",
    "    print(summary.round(3).head(12).to_string(index=False))\n",
    "\n",
    "    return sim, summary, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim, summary, df = test_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sorted = summary.sort_values([\"beta\", \"gamma\", \"network_type\"]).round(3)\n",
    "print(summary_sorted.to_string(index=False))\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting class\n",
    "\n",
    "class Plotter:\n",
    "\n",
    "    def __init__(self, sim, pretty_style=True):\n",
    "        self.sim = sim\n",
    "        if pretty_style:\n",
    "            plt.rcParams.update({\n",
    "                \"figure.dpi\": 110,\n",
    "                \"axes.grid\": True, \"grid.alpha\": 0.3,\n",
    "                \"axes.spines.top\": False, \"axes.spines.right\": False\n",
    "            })\n",
    "\n",
    "    def structure_overview(self):\n",
    "        \"\"\"\n",
    "        Summary table (mean ± std) + bar plots + degree histograms for ER/BA/WS.\n",
    "        Requires sim.compute_network_stats to include:\n",
    "        avg_degree, std_degree, density, avg_path_length, avg_clustering,\n",
    "        transitivity, tau_c_mean_over_second (⟨k⟩/⟨k²⟩)\n",
    "        \"\"\"\n",
    "        sim = self.sim\n",
    "        if not sim.networks:\n",
    "            raise RuntimeError(\"No networks stored\")\n",
    "\n",
    "        rows = []\n",
    "        for nt in sim.network_types:\n",
    "            for G in sim.networks[nt]:\n",
    "                s = sim.compute_network_stats(G)\n",
    "                s[\"network_type\"] = nt\n",
    "                rows.append(s)\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        cols = [\n",
    "            (\"avg_degree\", \"⟨k⟩\"),\n",
    "            (\"std_degree\", \"std(k)\"),\n",
    "            (\"density\", \"density\"),\n",
    "            (\"avg_path_length\", \"APL(LCC)\"),\n",
    "            (\"avg_clustering\", \"clustering\"),\n",
    "            (\"transitivity\", \"transitivity\"),\n",
    "            (\"tau_c_mean_over_second\", \"τc=⟨k⟩/⟨k²⟩\"),\n",
    "        ]\n",
    "\n",
    "        agg = df.groupby(\"network_type\")[[c for c, _ in cols]].agg([\"mean\", \"std\"])\n",
    "\n",
    "        pretty = pd.DataFrame(index=agg.index)\n",
    "        for c, label in cols:\n",
    "            m = agg[(c, \"mean\")]\n",
    "            sd = agg[(c, \"std\")]\n",
    "            if c in (\"avg_degree\", \"std_degree\", \"avg_path_length\"):\n",
    "                fmt = lambda x: f\"{x:.2f}\"\n",
    "            elif c in (\n",
    "                \"density\",\n",
    "                \"avg_clustering\",\n",
    "                \"transitivity\",\n",
    "                \"tau_c_mean_over_second\",\n",
    "            ):\n",
    "                fmt = lambda x: f\"{x:.4f}\"\n",
    "            else:\n",
    "                fmt = lambda x: f\"{x:.3f}\"\n",
    "            pretty[label] = [f\"{fmt(mi)}±{fmt(si)}\" for mi, si in zip(m, sd)]\n",
    "        pretty.index = [nt.replace(\"_\", \" \").title() for nt in pretty.index]\n",
    "\n",
    "        print(\"\\nNetwork Structure: Means ± SD across instances\")\n",
    "        print(pretty.to_string())\n",
    "\n",
    "        #  Bar plots (mean ± sd)\n",
    "        metrics = [\n",
    "            (\"avg_degree\", \"Average Degree\"),\n",
    "            (\"avg_path_length\", \"Average Path Length (LCC)\"),\n",
    "            (\"std_degree\", \"Degree Std Dev\"),\n",
    "            (\"density\", \"Density\"),\n",
    "            (\"avg_clustering\", \"Average Clustering\"),\n",
    "            (\"transitivity\", \"Transitivity\"),\n",
    "        ]\n",
    "        n_cols = 2\n",
    "        n_rows = int(np.ceil(len(metrics) / n_cols))\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        colors = {\n",
    "            \"erdos_renyi\": \"#8ecae6\",\n",
    "            \"barabasi_albert\": \"#bde0fe\",\n",
    "            \"watts_strogatz\": \"#ffc8dd\",\n",
    "        }\n",
    "\n",
    "        for ax, (key, label) in zip(axes, metrics):\n",
    "            g = df.groupby(\"network_type\")[key].agg([\"mean\", \"std\"])\n",
    "            x = np.arange(len(g))\n",
    "            ax.bar(\n",
    "                x,\n",
    "                g[\"mean\"].values,\n",
    "                yerr=g[\"std\"].values,\n",
    "                color=[colors[t] for t in g.index],\n",
    "                alpha=0.85,\n",
    "                capsize=5,\n",
    "            )\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([t.replace(\"_\", \" \").title() for t in g.index])\n",
    "            ax.set_ylabel(label)\n",
    "            ax.set_title(label)\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "        for j in range(len(metrics), len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        plt.suptitle(\"Network Structure Overview (mean ± std)\", fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Degree distributions\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        for ax, t in zip(axes, sim.network_types):\n",
    "            degrees = []\n",
    "            for G in sim.networks[t]:\n",
    "                degrees.extend([d for _, d in G.degree()])\n",
    "            ax.hist(degrees, bins=30, edgecolor=\"black\", alpha=0.75)\n",
    "            ax.set_title(t.replace(\"_\", \" \").title())\n",
    "            ax.set_xlabel(\"Degree\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "            # BA log–log inset for tail\n",
    "            if t == \"barabasi_albert\" and len(degrees) > 0:\n",
    "                counts = np.bincount(degrees)\n",
    "                k = np.arange(len(counts))\n",
    "                mask = counts > 0\n",
    "                inset = ax.inset_axes([0.55, 0.55, 0.4, 0.4])\n",
    "                inset.loglog(k[mask], counts[mask], \"o\", ms=3)\n",
    "                inset.set_title(\"Log–log\", fontsize=8)\n",
    "                inset.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # SIR Comparison\n",
    "\n",
    "    def sir_comparison(\n",
    "        self, beta=0.1, gamma=0.05, fraction_infected=0.02, iterations=220\n",
    "    ):\n",
    "        \"\"\"\n",
    "        One SIR run per network type (1st instance). Plots S, I, R counts over time with peak marker.\n",
    "        \"\"\"\n",
    "        sim = self.sim\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        for idx, net_type in enumerate(sim.network_types):\n",
    "            G = sim.networks[net_type][0]\n",
    "            res = sim.run_sir_simulation(\n",
    "                G,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "                fraction_infected=fraction_infected,\n",
    "                iterations=iterations,\n",
    "                seed=sim.seed,\n",
    "            )\n",
    "            trends = res[\"trends\"][0][\"trends\"][\"node_count\"]\n",
    "            ax = axes[idx]\n",
    "            ax.plot(trends[0], \"b-\", label=\"S\", linewidth=2)\n",
    "            ax.plot(trends[1], \"r-\", label=\"I\", linewidth=2)\n",
    "            ax.plot(trends[2], \"g-\", label=\"R\", linewidth=2)\n",
    "            ax.set_title(net_type.replace(\"_\", \" \").title())\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            peak_i = max(trends[1])\n",
    "            peak_t = trends[1].index(peak_i)\n",
    "            ax.plot(peak_t, peak_i, \"ro\", markersize=7)\n",
    "            ax.annotate(\n",
    "                f\"Peak: {peak_i}\",\n",
    "                xy=(peak_t, peak_i),\n",
    "                xytext=(peak_t + 5, peak_i + 5),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"red\", alpha=0.5),\n",
    "            )\n",
    "        fig.suptitle(f\"SIR Comparison (β={beta}, γ={gamma}, R₀={beta/gamma:.2f})\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def epidemic_snapshot(\n",
    "        self,\n",
    "        network_type: str,\n",
    "        instance: int,\n",
    "        beta: float,\n",
    "        gamma: float,\n",
    "        fraction_infected: float = 0.02,\n",
    "        iterations: int = 220,\n",
    "        t: int = 20,\n",
    "        layout: str = \"spring\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simulate once on the selected graph and draw the network at time t,\n",
    "        coloring nodes by S (blue), I (red), R (green).\n",
    "        \"\"\"\n",
    "        sim = self.sim\n",
    "        color_map = {0: \"#1f77b4\", 1: \"#d62728\", 2: \"#2ca02c\"}  # S, I, R\n",
    "        G = sim.networks[network_type][instance]\n",
    "        res = sim.run_sir_simulation(\n",
    "            G,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            fraction_infected=fraction_infected,\n",
    "            iterations=iterations,\n",
    "            seed=sim.seed,\n",
    "        )\n",
    "        iters = res[\"iterations\"]\n",
    "        t = max(0, min(t, len(iters) - 1))\n",
    "        status_dict = iters[t][\"status\"]  # {node: state_code}\n",
    "        colors = [color_map[status_dict.get(n, 0)] for n in G.nodes()]\n",
    "\n",
    "        if layout == \"spring\":\n",
    "            pos = nx.spring_layout(G, seed=sim.seed)\n",
    "        elif layout == \"kamada_kawai\":\n",
    "            pos = nx.kamada_kawai_layout(G)\n",
    "        elif layout == \"circular\":\n",
    "            pos = nx.circular_layout(G)\n",
    "        else:\n",
    "            pos = nx.spring_layout(G, seed=sim.seed)\n",
    "\n",
    "        plt.figure(figsize=(6.5, 6.0))\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos, node_color=colors, node_size=30, linewidths=0.2, edgecolors=\"k\"\n",
    "        )\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.25, width=0.7)\n",
    "        plt.title(\n",
    "            f\"{network_type.replace('_',' ').title()} snapshot @ t={t} (β={beta}, γ={gamma})\"\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # Parameter sweep plots\n",
    "\n",
    "    def parameter_sweep(self, df: pd.DataFrame):\n",
    "        \"\"\"Sumamry using experiment_results:\"\"\"\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            print(\"No experiment results.\")\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "        # Peak vs R0\n",
    "        ax = axes[0, 0]\n",
    "        for net_type in self.sim.network_types:\n",
    "            data = df[df[\"network_type\"] == net_type]\n",
    "            grouped = data.groupby(\"R0_naive\").agg({\"peak_frac\": [\"mean\", \"std\"]})\n",
    "            x = grouped.index\n",
    "            y = grouped[\"peak_frac\"][\"mean\"]\n",
    "            yerr = grouped[\"peak_frac\"][\"std\"]\n",
    "            ax.errorbar(x, y, yerr=yerr, marker=\"o\", label=net_type, capsize=5)\n",
    "        ax.set_xlabel(\"R0 = β/γ\")\n",
    "        ax.set_ylabel(\"Peak Infected Fraction\")\n",
    "        ax.set_title(\"Peak Infection vs R0\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Time to peak box\n",
    "        ax = axes[0, 1]\n",
    "        data_box = [\n",
    "            df[df[\"network_type\"] == t][\"time_to_peak\"].values\n",
    "            for t in self.sim.network_types\n",
    "        ]\n",
    "        ax.boxplot(\n",
    "            data_box,\n",
    "            labels=[t.replace(\"_\", \" \").title() for t in self.sim.network_types],\n",
    "        )\n",
    "        ax.set_ylabel(\"Time to Peak\")\n",
    "        ax.set_title(\"Speed of Epidemic Peak\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Heatmap of peak\n",
    "        ax = axes[1, 0]\n",
    "        pivot = df.pivot_table(\n",
    "            values=\"peak_frac\", index=\"gamma\", columns=\"beta\", aggfunc=\"mean\"\n",
    "        )\n",
    "        im = ax.imshow(\n",
    "            pivot,\n",
    "            cmap=\"YlOrRd\",\n",
    "            aspect=\"auto\",\n",
    "            extent=[\n",
    "                pivot.columns.min(),\n",
    "                pivot.columns.max(),\n",
    "                pivot.index.min(),\n",
    "                pivot.index.max(),\n",
    "            ],\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "        ax.set_xlabel(\"β\")\n",
    "        ax.set_ylabel(\"γ\")\n",
    "        ax.set_title(\"Peak Infection Fraction Heatmap\")\n",
    "        plt.colorbar(im, ax=ax)\n",
    "\n",
    "        # Final size vs R0\n",
    "        ax = axes[1, 1]\n",
    "        for net_type in self.sim.network_types:\n",
    "            data = df[df[\"network_type\"] == net_type]\n",
    "            ax.scatter(\n",
    "                data[\"R0_naive\"],\n",
    "                data[\"final_size_frac\"],\n",
    "                alpha=0.5,\n",
    "                label=net_type,\n",
    "                s=20,\n",
    "            )\n",
    "        ax.axvline(x=1, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "        ax.set_xlabel(\"R0 = β/γ\")\n",
    "        ax.set_ylabel(\"Final Epidemic Size\")\n",
    "        ax.set_title(\"R0 vs Final Size\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Intiial Condition Effects\n",
    "\n",
    "    def fraction_effects(self, df_ic, metric=\"peak_frac\", show_strategies=True):\n",
    "        \"\"\"\n",
    "        Summarize how outcomes change with initial infected fraction.\n",
    "        Default shows separate lines per strategy within each network type.\n",
    "        Set show_strategies=False to average over strategies.\n",
    "        \"\"\"\n",
    "        if df_ic is None or df_ic.empty:\n",
    "            print(\"No data to plot\")\n",
    "            return\n",
    "\n",
    "        frac_col = (\n",
    "            \"fraction\"\n",
    "            if \"fraction\" in df_ic.columns\n",
    "            else (\"seed_fraction\" if \"seed_fraction\" in df_ic.columns else None)\n",
    "        )\n",
    "        if frac_col is None:\n",
    "            raise KeyError(\"Expected a 'fraction' or 'seed_fraction' column in df_ic.\")\n",
    "\n",
    "        if metric not in df_ic.columns:\n",
    "            raise KeyError(\n",
    "                f\"Metric '{metric}' not found in df_ic. Available: {list(df_ic.columns)}\"\n",
    "            )\n",
    "\n",
    "        df_ic = df_ic.copy()\n",
    "        df_ic[frac_col] = pd.to_numeric(df_ic[frac_col], errors=\"coerce\")\n",
    "\n",
    "        if not show_strategies:\n",
    "            g = (\n",
    "                df_ic.groupby([\"network_type\", frac_col])[metric]\n",
    "                .agg([\"mean\", \"std\"])\n",
    "                .reset_index()\n",
    "            )\n",
    "            fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "            for nt, sub in g.groupby(\"network_type\"):\n",
    "                sub = sub.sort_values(frac_col)\n",
    "                ax.errorbar(\n",
    "                    sub[frac_col],\n",
    "                    sub[\"mean\"],\n",
    "                    yerr=sub[\"std\"],\n",
    "                    marker=\"o\",\n",
    "                    capsize=4,\n",
    "                    label=nt,\n",
    "                )\n",
    "            ax.set_xlabel(\"Initial infected fraction\")\n",
    "            ax.set_ylabel(metric.replace(\"_\", \" \").title())\n",
    "            ax.set_title(\n",
    "                f'{metric.replace(\"_\",\" \").title()} vs Initial Fraction (averaged over strategies)'\n",
    "            )\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            nts = sorted(df_ic[\"network_type\"].unique())\n",
    "            fig, axes = plt.subplots(\n",
    "                1, len(nts), figsize=(5 * len(nts), 4), sharey=True\n",
    "            )\n",
    "            if len(nts) == 1:\n",
    "                axes = [axes]\n",
    "            for ax, nt in zip(axes, nts):\n",
    "                sub = df_ic[df_ic[\"network_type\"] == nt]\n",
    "                g = (\n",
    "                    sub.groupby([\"strategy\", frac_col])[metric]\n",
    "                    .agg([\"mean\", \"std\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "                for strat, sub2 in g.groupby(\"strategy\"):\n",
    "                    sub2 = sub2.sort_values(frac_col)\n",
    "                    ax.errorbar(\n",
    "                        sub2[frac_col],\n",
    "                        sub2[\"mean\"],\n",
    "                        yerr=sub2[\"std\"],\n",
    "                        marker=\"o\",\n",
    "                        capsize=4,\n",
    "                        label=strat.replace(\"_\", \" \"),\n",
    "                    )\n",
    "                ax.set_title(nt.replace(\"_\", \" \").title())\n",
    "                ax.set_xlabel(\"Initial infected fraction\")\n",
    "                ax.grid(True, alpha=0.3)\n",
    "            axes[0].set_ylabel(metric.replace(\"_\", \" \").title())\n",
    "            handles, labels = axes[-1].get_legend_handles_labels()\n",
    "            fig.legend(handles, labels, loc=\"upper center\", ncol=3)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = NetworkSimulator(seed=42)\n",
    "sim.generate_networks(\n",
    "    n_nodes=400,\n",
    "    n_instances=5,\n",
    "    target_k=5.0,\n",
    "    ws_p=0.15,            # small-world shortcuts\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Plotter(sim, pretty_style=True)\n",
    "plot.structure_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.sir_comparison(beta=0.10, gamma=0.05, fraction_infected=0.02, iterations=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas  = [0.05, 0.08, 0.12, 0.18]\n",
    "gammas = [0.03, 0.05, 0.08]\n",
    "df_sweep = sim.experiment_parameter_sweep(\n",
    "    n_repetitions=3,\n",
    "    beta_values=betas,\n",
    "    gamma_values=gammas\n",
    ")\n",
    "\n",
    "print(df_sweep.head(8))\n",
    "plot.parameter_sweep(df_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = [0.005, 0.01, 0.02, 0.05]   # 0.5% to 5%\n",
    "df_ic = sim.experiment_initial_conditions(\n",
    "    beta=0.06, gamma=0.05,               # near-threshold: structure differences visible\n",
    "    iterations=220,\n",
    "    fractions=fractions,\n",
    "    repetitions=5,\n",
    ")\n",
    "print(df_ic.groupby(['network_type','strategy'])[['peak_frac','time_to_peak','final_size_frac']].mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.fraction_effects(df_ic, metric='peak_frac')     \n",
    "plot.fraction_effects(df_ic, metric='time_to_peak')\n",
    "plot.fraction_effects(df_ic, metric='final_size_frac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of network image at a some t after outbreak start\n",
    "\n",
    "plot.epidemic_snapshot(\n",
    "    network_type='barabasi_albert', instance=0,\n",
    "    beta=0.12, gamma=0.05,\n",
    "    fraction_infected=0.1,\n",
    "    iterations=220, t=8,\n",
    "    layout='kamada_kawai'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sociopatterns Dataset\n",
    "\n",
    "The sociopatterns dataset has 374 nodes and 1265 egdes. The nodes are people and edges exist if two people spent time near each other during the conference.\n",
    "\n",
    "NOTE: Disregard weights that indicae how long people were in range of one another. Assume all contacts involve equal length and hence equal chance of transmission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sociopatterns] N=354, L=1263, ⟨k⟩=7.14\n"
     ]
    }
   ],
   "source": [
    "sim = NetworkSimulator(seed=42)\n",
    "\n",
    "# Load the dataset\n",
    "Gsp = sim.load_sociopatterns(\n",
    "    \"data/transmission_network.csv\",  # path\n",
    "    threshold=0.0,        \n",
    "    giant_component=True, # keep only the main connected cluster\n",
    "    relabel_to_int=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 354\n",
      "Edges: 1263\n"
     ]
    }
   ],
   "source": [
    "print(\"Nodes:\", Gsp.number_of_nodes())\n",
    "print(\"Edges:\", Gsp.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = Plotter(sim)\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.kamada_kawai_layout(Gsp)       # good for human contact networks\n",
    "nx.draw_networkx_nodes(Gsp, pos, node_size=25, node_color=\"#4C72B0\", alpha=0.9)\n",
    "nx.draw_networkx_edges(Gsp, pos, alpha=0.25, width=0.6)\n",
    "deg = [d for _, d in Gsp.degree()]\n",
    "nx.draw_networkx_nodes(Gsp, pos, node_color=deg, cmap=\"viridis\",\n",
    "                       node_size=25, alpha=0.9)\n",
    "plt.title(\"Filtered Sociopatterns Contact Network (N=374, L=1265)\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Analyze network structure to implement vaccination strategy.\n",
    "\"\"\"\n",
    "\n",
    "Gsp = sim.networks['sociopatterns'][0]\n",
    "stats = sim.compute_network_stats(Gsp)\n",
    "\n",
    "for k in [\n",
    "    \"n_nodes\",\"n_edges\",\"avg_degree\",\"std_degree\",\"min_degree\",\"max_degree\",\n",
    "    \"density\",\"avg_clustering\",\"transitivity\",\"avg_path_length\",\"diameter\",\n",
    "    \"second_moment_k2\",\"tau_c_mean_over_second\",\"degree_heterogeneity\",\"avg_betweenness\"\n",
    "]:\n",
    "    print(f\"{k:22s} : {stats.get(k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements:\n",
    "\n",
    "1. Dynamic vaccination strategy with a testing budget and a limited number of vaccinations available per iteration of the model.\n",
    "\n",
    "2. Maximum number of tests per iteration of the model, but can use less\n",
    "\n",
    "3. Assume network structure is known, but the infection status of an individual is only known after a test.\n",
    "\n",
    "4. Vaccinations applied to the S class move them immediately to the removed class\n",
    "\n",
    "5. No waning immunity\n",
    "\n",
    "6. Compare the strategies with different budgets of (1, 3, 5, and 10) per timestep and different testing accuracy (0.5, 0.75, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives: 4, True positives: 0\n",
      "Ring Vaccination Enhanced  -> peak=0.410, final=0.944\n",
      "Random Vaccination Control -> peak=0.494, final=1.000\n"
     ]
    }
   ],
   "source": [
    "Gsp = sim.networks[\"sociopatterns\"][0]\n",
    "beta, gamma = 0.12, 0.05\n",
    "\n",
    "mixed = sim.experiment_vaccination_mixed(\n",
    "    Gsp,\n",
    "    beta=beta,\n",
    "    gamma=gamma,\n",
    "    tests_per_step=5,\n",
    "    vaccines_per_step=5,\n",
    "    test_accuracy=0.75,\n",
    "    iterations=300,\n",
    "    seed_inits=5,\n",
    "    seed=2026,\n",
    ")\n",
    "random = sim.experiment_vaccination_random(\n",
    "    Gsp,\n",
    "    beta=beta,\n",
    "    gamma=gamma,\n",
    "    tests_per_step=5,\n",
    "    vaccines_per_step=5,\n",
    "    test_accuracy=0.75,\n",
    "    iterations=300,\n",
    "    seed_inits=5,\n",
    "    seed=2027,\n",
    ")\n",
    "\n",
    "print(\"Ring Vaccination Enhanced  -> peak={:.3f}, final={:.3f}\".format(mixed[\"peak_frac\"], mixed[\"final_size_frac\"]))\n",
    "print(\"Random Vaccination Control -> peak={:.3f}, final={:.3f}\".format(random[\"peak_frac\"], random[\"final_size_frac\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 1\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 5, True positives: 1\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 7, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 8, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 1, True positives: 1\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 8, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 9, True positives: 0\n",
      "False positives: 9, True positives: 0\n",
      "False positives: 10, True positives: 0\n",
      "False positives: 11, True positives: 0\n",
      "False positives: 7, True positives: 1\n",
      "False positives: 6, True positives: 1\n",
      "False positives: 9, True positives: 0\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 12, True positives: 1\n",
      "False positives: 9, True positives: 0\n",
      "False positives: 7, True positives: 0\n",
      "False positives: 8, True positives: 0\n",
      "False positives: 7, True positives: 0\n",
      "False positives: 7, True positives: 1\n",
      "False positives: 7, True positives: 0\n",
      "False positives: 8, True positives: 0\n",
      "False positives: 5, True positives: 1\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 7, True positives: 0\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 8, True positives: 0\n",
      "False positives: 8, True positives: 1\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 8, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 7, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 7, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 4, True positives: 0\n",
      "False positives: 5, True positives: 1\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 2, True positives: 1\n",
      "False positives: 5, True positives: 0\n",
      "False positives: 3, True positives: 1\n",
      "False positives: 3, True positives: 1\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 1, True positives: 1\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 2, True positives: 0\n",
      "False positives: 4, True positives: 1\n",
      "False positives: 6, True positives: 0\n",
      "False positives: 4, True positives: 1\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 3, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 1, True positives: 0\n",
      "False positives: 2, True positives: 1\n",
      "False positives: 0, True positives: 1\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 1\n",
      "False positives: 0, True positives: 1\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 1\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 3\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n",
      "False positives: 0, True positives: 0\n"
     ]
    }
   ],
   "source": [
    "beta, gamma = 0.12, 0.05  # R0 ~ 2.4\n",
    "df = sim.sweep_dynamic_vaccination(\n",
    "    Gsp, beta, gamma,\n",
    "    budgets=(1,3,5,10),\n",
    "    accs=(0.5,0.75,1.0),\n",
    "    iterations=300,\n",
    "    reps=30,               # start with 30; bump to 50+ for your final\n",
    "    seed0=2025\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df, metric=\"final_size_frac\"):\n",
    "    g = (df.groupby([\"policy\",\"tests_per_step\",\"test_accuracy\"])[metric]\n",
    "           .agg([\"mean\",\"std\",\"count\"])\n",
    "           .reset_index())\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest final sizes:\n",
      "    policy  tests_per_step  test_accuracy  final_size_frac  peak_frac\n",
      "542    RPA              10            0.5         0.757062   0.166667\n",
      "548    RPA              10            0.5         0.759887   0.158192\n",
      "584    RPA              10            0.5         0.762712   0.200565\n",
      "562    RPA              10            0.5         0.765537   0.172316\n",
      "558    RPA              10            0.5         0.765537   0.206215\n"
     ]
    }
   ],
   "source": [
    "best = df.sort_values(\"final_size_frac\").head(5)\n",
    "print(\"Lowest final sizes:\")\n",
    "print(best[[\"policy\",\"tests_per_step\",\"test_accuracy\",\"final_size_frac\",\"peak_frac\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  policy  tests_per_step  test_accuracy  peak_frac  final_size_frac\n",
      "0    RPA               1           0.50   0.626554         0.982203\n",
      "1    RPA               1           0.75   0.620621         0.978531\n",
      "2    RPA               1           1.00   0.625424         0.981450\n",
      "3    RPA               3           0.50   0.514878         0.956026\n",
      "4    RPA               3           0.75   0.506968         0.958475\n",
      "5    RPA               3           1.00   0.524294         0.959981\n",
      "6    RPA               5           0.50   0.383427         0.916949\n",
      "7    RPA               5           0.75   0.405556         0.923823\n",
      "8    RPA               5           1.00   0.413465         0.928249\n",
      "9    RPA              10           0.50   0.173258         0.795951\n"
     ]
    }
   ],
   "source": [
    "# grouped mean summary\n",
    "summary = (df.groupby([\"policy\",\"tests_per_step\",\"test_accuracy\"])\n",
    "              [[\"peak_frac\",\"final_size_frac\"]].mean().reset_index())\n",
    "print(summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">final_size_frac</th>\n",
       "      <th colspan=\"3\" halign=\"left\">peak_frac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policy</th>\n",
       "      <th>tests_per_step</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">RPA</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0.50</th>\n",
       "      <td>0.985</td>\n",
       "      <td>0.010</td>\n",
       "      <td>30</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.029</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.009</td>\n",
       "      <td>30</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.034</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.984</td>\n",
       "      <td>0.009</td>\n",
       "      <td>30</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.024</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>0.50</th>\n",
       "      <td>0.962</td>\n",
       "      <td>0.011</td>\n",
       "      <td>30</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.031</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.963</td>\n",
       "      <td>0.009</td>\n",
       "      <td>30</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.042</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.011</td>\n",
       "      <td>30</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.026</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>0.50</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.014</td>\n",
       "      <td>30</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.029</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.928</td>\n",
       "      <td>0.015</td>\n",
       "      <td>30</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.037</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.940</td>\n",
       "      <td>0.014</td>\n",
       "      <td>30</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.035</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">10</th>\n",
       "      <th>0.50</th>\n",
       "      <td>0.799</td>\n",
       "      <td>0.024</td>\n",
       "      <td>30</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.040</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.817</td>\n",
       "      <td>0.023</td>\n",
       "      <td>30</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.044</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.838</td>\n",
       "      <td>0.022</td>\n",
       "      <td>30</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.040</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">random</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0.50</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.003</td>\n",
       "      <td>30</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.026</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.005</td>\n",
       "      <td>30</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.032</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.007</td>\n",
       "      <td>30</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.024</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>0.50</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.003</td>\n",
       "      <td>30</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.028</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.026</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.002</td>\n",
       "      <td>30</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.031</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>0.50</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.028</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.034</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.029</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">10</th>\n",
       "      <th>0.50</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.023</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.026</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>30</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.035</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    final_size_frac              peak_frac  \\\n",
       "                                               mean    std count      mean   \n",
       "policy tests_per_step test_accuracy                                          \n",
       "RPA    1              0.50                    0.985  0.010    30     0.631   \n",
       "                      0.75                    0.981  0.009    30     0.627   \n",
       "                      1.00                    0.984  0.009    30     0.630   \n",
       "       3              0.50                    0.962  0.011    30     0.537   \n",
       "                      0.75                    0.963  0.009    30     0.543   \n",
       "                      1.00                    0.966  0.011    30     0.552   \n",
       "       5              0.50                    0.923  0.014    30     0.439   \n",
       "                      0.75                    0.928  0.015    30     0.448   \n",
       "                      1.00                    0.940  0.014    30     0.474   \n",
       "       10             0.50                    0.799  0.024    30     0.210   \n",
       "                      0.75                    0.817  0.023    30     0.250   \n",
       "                      1.00                    0.838  0.022    30     0.284   \n",
       "random 1              0.50                    0.996  0.003    30     0.635   \n",
       "                      0.75                    0.996  0.005    30     0.633   \n",
       "                      1.00                    0.996  0.007    30     0.639   \n",
       "       3              0.50                    0.999  0.003    30     0.588   \n",
       "                      0.75                    0.999  0.001    30     0.596   \n",
       "                      1.00                    0.999  0.002    30     0.590   \n",
       "       5              0.50                    0.999  0.001    30     0.541   \n",
       "                      0.75                    1.000  0.001    30     0.540   \n",
       "                      1.00                    1.000  0.000    30     0.533   \n",
       "       10             0.50                    1.000  0.001    30     0.420   \n",
       "                      0.75                    1.000  0.001    30     0.437   \n",
       "                      1.00                    1.000  0.002    30     0.427   \n",
       "\n",
       "                                                  \n",
       "                                       std count  \n",
       "policy tests_per_step test_accuracy               \n",
       "RPA    1              0.50           0.029    30  \n",
       "                      0.75           0.034    30  \n",
       "                      1.00           0.024    30  \n",
       "       3              0.50           0.031    30  \n",
       "                      0.75           0.042    30  \n",
       "                      1.00           0.026    30  \n",
       "       5              0.50           0.029    30  \n",
       "                      0.75           0.037    30  \n",
       "                      1.00           0.035    30  \n",
       "       10             0.50           0.040    30  \n",
       "                      0.75           0.044    30  \n",
       "                      1.00           0.040    30  \n",
       "random 1              0.50           0.026    30  \n",
       "                      0.75           0.032    30  \n",
       "                      1.00           0.024    30  \n",
       "       3              0.50           0.028    30  \n",
       "                      0.75           0.026    30  \n",
       "                      1.00           0.031    30  \n",
       "       5              0.50           0.028    30  \n",
       "                      0.75           0.034    30  \n",
       "                      1.00           0.029    30  \n",
       "       10             0.50           0.023    30  \n",
       "                      0.75           0.026    30  \n",
       "                      1.00           0.035    30  "
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.groupby([\"policy\",\"tests_per_step\",\"test_accuracy\"])\n",
    "   [[\"final_size_frac\",\"peak_frac\"]]\n",
    "   .agg([\"mean\",\"std\",\"count\"])\n",
    "   .round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
